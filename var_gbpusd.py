# -*- coding: utf-8 -*-
"""VAR_GBPUSD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QskbGLejzI9Y-IeXR7WG_qVDkZeeWXoO

# Initialization
"""

# connect to drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# import
import os
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

# libraries
from datetime import datetime
import re
# %matplotlib inline
import numpy as np
import pandas as pd
import scipy
from datetime import datetime, timedelta
import statsmodels.api as sm
from statsmodels.tsa.api import VAR
import matplotlib.pyplot as plt
import pandas_datareader as pdr
import math

"""# Read row data, arrange, observe descriptive statistics"""

# read data from google drive
data = pd.read_excel('/content/drive/MyDrive/Rady_for_R_GBPUSD_EPU.xlsx', parse_dates =['Date'])

data.head()

data['Date'] = pd.to_datetime(data['Date'])
data['USDGBP'] = 1/data['USDGBP']
data.rename(columns={'USDGBP':'GBPUSD'}, inplace=True)
#data.info()

#Declear that it is a time series data
data.index = data.Date
data = data.drop('Date', axis=1)
data

data.describe()

# data['News_Sum'] = data[['Stock Market News', 'Economic Development News', 'FED News', 'Micro Finance News', 'International Trade News']].sum(axis=1)
# # Plot the sum over time
# plt.figure(figsize=(10, 6))
# plt.plot(data.index, data['News_Sum'], label='Sum of News Columns')
# plt.xlabel('Date')
# plt.ylabel('Sum of News Variables')
# plt.title('Sum of News Columns Over Time')
# plt.legend()
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()

"""# Plot Monthly Entropies"""

# from google.colab import files
# # Define the topics and their corresponding DataFrame columns
# topiccolors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]

# topics = {
#     "Stock Market": "Stock Market News",
#     "Economic Development": "Economic Development News",
#     "ECB": "FED News",
#     "Microeconomics": "Micro Finance News",
#     "International Trade": "International Trade News"
# }

# # Create a plot for each topic
# for i, (topic, column) in enumerate(topics.items(), start=1):
#     fig, ax = plt.subplots(figsize=(11.69, 8.27), dpi=600)  # A4 size in inches
#     ax.plot(data.index, data[column], color=topiccolors[i-1])  # Use index for time

#     plt.gcf().autofmt_xdate()  # Format x-axis dates
#     plt.xlabel('Time', fontsize=16)
#     plt.ylabel(f'Monthly Entropy to the Topic \n {topic}', fontsize=16)

#     # Set y-ticks to three decimal places
#     ax.set_yticks(ax.get_yticks())  # Get current ticks
#     ax.set_yticklabels([f'{tick:.3f}' for tick in ax.get_yticks()], fontsize=16)  # Format to three decimal places

#    # Set x-ticks to show only years
#     num_indices = len(data.index)  # Total number of indices
#     ax.set_xticks(data.index[::12])  # Set x-ticks to every 13th value
#     ax.set_xticklabels(data.index[::12].strftime('%Y'), rotation=45, ha='right', fontsize=16)
#     # Add gray grid
#     ax.grid(color='gray', linestyle='--', linewidth=0.7, alpha=0.7)  # Customize grid appearance

#     plt.title(f'Monthly Attention to the Topic {i}\n {topic}', fontsize=20)

#     plt.tight_layout()  # Adjust layout to fit labels
#     plt.savefig(f'Monthly_Entropy_to_the_Topic_{topic.replace(" ", "_")}.pdf')  # Save as PDF
#     plt.show()
#     files.download(f'Monthly_Entropy_to_the_Topic_{topic.replace(" ", "_")}.pdf')
#     plt.close()  # Close the plot to free memory

"""# Alternative Measures - Perplexity

"""

# import pandas as pd
# import numpy as np

# # 1. **Mutual Information** between two entropy columns
# def mutual_information(entropy_x, entropy_y, joint_entropy):
#     return entropy_x + entropy_y - joint_entropy

# # 2. **KL Divergence (Relative Entropy)** between two distributions
# def kl_divergence(p, q):
#     return np.sum(p * np.log(p / q))

# # 3. **Conditional Entropy** H(X|Y) = H(X, Y) - H(Y)
# def conditional_entropy(entropy_x, joint_entropy_xy, entropy_y):
#     return joint_entropy_xy - entropy_y

# # 4. **Perplexity** (2^H)
# def perplexity(entropy_value):
#     return np.power(2, entropy_value)

# # Example: Calculate new variables from the Shannon entropy values
# # Mutual Information between 'Stock Market News' and 'Economic Development News'
# joint_entropy_stock_dev = 1.5  # Example joint entropy value (replace with actual value if needed)
# mi_stock_dev = mutual_information(data['Stock Market News'].mean(), data['Economic Development News'].mean(), joint_entropy_stock_dev)

# # KL Divergence between 'Stock Market News' and 'Economic Development News'
# kl_div_value = kl_divergence(data['Stock Market News'], data['Economic Development News'])

# # Conditional Entropy for 'Stock Market News' given 'Economic Development News'
# joint_entropy_stock_dev = 1.5  # Example joint entropy value
# cond_entropy_stock_dev = conditional_entropy(data['Stock Market News'].mean(), joint_entropy_stock_dev, data['Economic Development News'].mean())

# # Perplexity for 'Stock Market News'
# #perplexity_stock_market_news = perplexity(data['Stock Market News'])

# # Create new variables
# #data['Mutual Information Stock-EconDev'] = mi_stock_dev
# #data['KL Divergence Stock-EconDev'] = kl_div_value
# #data['Conditional Entropy Stock-EconDev'] = cond_entropy_stock_dev
# #data['Perplexity Stock Market News'] = perplexity_stock_market_news

# # Calculate Perplexity for all entropy columns in the data
# for col in data.columns:
#     if 'News' in col:  # Calculate Perplexity only for columns with entropy values
#         #data[f'Perplexity {col}'] = perplexity(data[col]) #adds in the end and then one should delete old variables data = data.drop(columns=data.columns[-12:-8])
#         data[f'{col}'] = perplexity(data[col]) #replaces existing variable with perplexity
# # Print the updated dataframe with new variables
# print("Updated Data with New Variables (Mutual Information, KL Divergence, Conditional Entropy, Perplexity):")
# print(data)

# from google.colab import files
# # Define the topics and their corresponding DataFrame columns
# topiccolors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]

# topics = {
#     "Stock Market": "Stock Market News",
#     "Economic Development": "Economic Development News",
#     "ECB": "FED News",
#     "Microeconomics": "Micro Finance News",
#     "International Trade": "International Trade News"
# }

# # Create a plot for each topic
# for i, (topic, column) in enumerate(topics.items(), start=1):
#     fig, ax = plt.subplots(figsize=(11.69, 8.27), dpi=600)  # A4 size in inches
#     ax.plot(data.index, data[column], color=topiccolors[i-1])  # Use index for time

#     plt.gcf().autofmt_xdate()  # Format x-axis dates
#     plt.xlabel('Time', fontsize=16)
#     plt.ylabel(f'Monthly Polarity to the Topic \n {topic}', fontsize=16)

#     # Set y-ticks to three decimal places
#     ax.set_yticks(ax.get_yticks())  # Get current ticks
#     ax.set_yticklabels([f'{tick:.3f}' for tick in ax.get_yticks()], fontsize=16)  # Format to three decimal places

#    # Set x-ticks to show only years
#     num_indices = len(data.index)  # Total number of indices
#     ax.set_xticks(data.index[::12])  # Set x-ticks to every 13th value
#     ax.set_xticklabels(data.index[::12].strftime('%Y'), rotation=45, ha='right', fontsize=16)
#     # Add gray grid
#     ax.grid(color='gray', linestyle='--', linewidth=0.7, alpha=0.7)  # Customize grid appearance

#     plt.title(f'Monthly Attention to the Topic {i}\n {topic}', fontsize=20)

#     plt.tight_layout()  # Adjust layout to fit labels
#     plt.savefig(f'Monthly_polarity_to_the_Topic_{topic.replace(" ", "_")}.pdf')  # Save as PDF
#     plt.show()
#     files.download(f'Monthly_polarity_to_the_Topic_{topic.replace(" ", "_")}.pdf')
#     plt.close()  # Close the plot to free memory

"""# Johansen Test on row data

"""

import numpy as np
import pandas as pd
from statsmodels.tsa.stattools import coint
# Define the pairs of variables for cointegration testing
pairs = [
    ('GBPUSD', 'Stock Market News'),
    ('GBPUSD', 'Economic Development News'),
    ('GBPUSD', 'FED News'),
    ('GBPUSD', 'M2_US'),
    ('GBPUSD', 'EPU_US'),
    ('GBPUSD', 'EPU_UK')
]

# Store the p-values for each pair of variables
p_values = {}

# Perform cointegration tests for each pair
for pair in pairs:
    result_coint = coint(data[pair[0]], data[pair[1]])
    p_values[pair] = result_coint[1]

# Print the p-values for each pair of variables
for pair, p_value in p_values.items():
    print(f"Cointegration test for {pair}: p-value = {p_value}")
#p_values>0.05, weak evidence against the null hypothesis of no cointegration. i.e. these news and GBPUSD cointegrate!

######Please referr to r code for the full version of johansen test with 16 variables. Here we check only 12 variables as critical values are calculated only for maximum 12 variables, for demonstrating.

#here we check only 12 variables, referr to r for the full version with 16 variables

from statsmodels.tsa.vector_ar.vecm import coint_johansen

# Assuming 'data' is your DataFrame containing the variables

# Perform the Johansen cointegration test
result = coint_johansen(data.iloc[:,:12], det_order=0, k_ar_diff=1)

# Extract the test statistics and critical values
test_statistics = result.lr1
critical_values = result.cvt

# Print the test statistics and critical values
print("Test statistics:", test_statistics)
print("Critical values:\n", critical_values)
# Compare test statistics to critical values
test_vs_critical = test_statistics[:, np.newaxis] > critical_values[:, 1]
# Calculate the p-values
p_values = (test_vs_critical).mean(axis=1)

# Print the p-values
print("P-values:", p_values)

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.vector_ar.vecm import coint_johansen

# Assuming 'data' is your DataFrame containing the time series data

# Step 1: Perform Johansen cointegration test
def johansen_cointegration_test(data):
    result = coint_johansen(data, det_order=0, k_ar_diff=1)
    print("Eigenvalues:", result.eig)
    print("Trace Statistic:", result.lr1)
    print("Critical Values (90%, 95%, 99%):", result.cvt)

    # Get the number of cointegrating vectors (rank) using maximum eigenvalue test
    rank_max_eig = result.ind
    print("Rank (using max eigenvalue test):", rank_max_eig)

    # Get the number of cointegrating vectors (rank) using trace test
    rank_trace = result.lr2
    print("Rank (using trace test):", rank_trace)

    return result

johansen_result = johansen_cointegration_test(data)

#This suggests that there is insufficient evidence to conclude that the residuals of johansen cointegration test are stationary. i.e. we do should go for VAR model!

#This suggests that there is insufficient evidence to conclude that the residuals of johansen cointegration test are stationary. i.e. we do should go for VAR model!

from statsmodels.tsa.stattools import adfuller

jotest = coint_johansen(data, det_order=0, k_ar_diff=6) #4 2
# Print the summary
#print("Eigenvalues:")
#print(jotest.eig)
#print("\nEigenvectors:")
#print(jotest.evec)
#print("\nCritical values (90%, 95%, 99%):")
#print(jotest.cvt)
#print("\nTrace statistic and critical values:")
#print("Trace statistic:", jotest.lr1)
#print("Critical values (90%, 95%, 99%):", jotest.cvm)
#print("\nMaximum eigenvalue statistic and critical values:")
#print("Maximum eigenvalue statistic:", jotest.lr2)
#print("Critical values (90%, 95%, 99%):", jotest.cvm)
#first_eigenvector = [2.20705422e+01, 1.96067863e+00, -2.03628052e+01, -2.48325292e+01, -2.60014714e+01, -1.05021603e+00, -1.66888390e+01, 9.73382101e+00, 1.45157187e+00, -2.85781928e+01, 1.18417956e+01, -1.18091693e+01, -1.12779071e+00, 1.21216745e+00, -2.03628309e+00, 5.88940695e+00]
#as there is a bag in r adfuller test, we conduct the adfuller test here.

w = (1.000000 * data.iloc[:, 0] + 802.1769 * data.iloc[:, 1] - 572.4314 * data.iloc[:, 2] - 0.03954655 * data.iloc[:, 3] + 37.72297 * data.iloc[:, 4] + 144.4919 * data.iloc[:, 5] - 142.5813 * data.iloc[:, 6] + 0.01644418 * data.iloc[:, 7] + 0.00001548652 * data.iloc[:, 8] - 8312.689 * data.iloc[:, 9] - 1706.767 * data.iloc[:, 10] + 1003.942 * data.iloc[:, 11] - 919.0407 * data.iloc[:, 12] - 234.6682 * data.iloc[:, 13] - 0.01478644 * data.iloc[:, 14] - 0.003823252 * data.iloc[:, 15]) #for 6 lags
#w = (1.000000 * data.iloc[:, 0] - 4.507364 * data.iloc[:, 1] + 1.239260 * data.iloc[:, 2] - 0.078895 * data.iloc[:, 3] - 0.031615 * data.iloc[:, 4] - 0.267858 * data.iloc[:, 5] + 0.166605 * data.iloc[:, 6] - 0.000028 * data.iloc[:, 7] + 0.000004 * data.iloc[:, 8] - 5.108912 * data.iloc[:, 9] + 0.280642 * data.iloc[:, 10] + 36.538940 * data.iloc[:, 11] - 30.943740 * data.iloc[:, 12] + 3.264700 * data.iloc[:, 13]) #for 4 lags

# Convert the calculated series to a pandas Series
w_ts = pd.Series(w)

# Assuming Monthly_UK_to_USD_ts is your time series data for comparison
# Perform ADF test
adf_result = adfuller(data.iloc[:, 0].values - w_ts.values)
print("ADF Statistic:", adf_result[0])
print("p-value:", adf_result[1])
print("Critical Values:", adf_result[4])
#This p-value >0.05, and suggests that there is insufficient evidence to conclude that the data is stationary.

"""# Data Cleaning"""

# calculate returns of exchange rate (log FX-log of lagFX)
#data['USDGBP'] = np.log(data['USDGBP'])-np.log(data['USDGBP'].shift(1))
#data

# Take LOGS
data['GBPUSD'] = np.log(data['GBPUSD'])
data['CPI_US'] = np.log(data['CPI_US'])
data['CPI_UK'] = np.log(data['CPI_UK'])
#data['Money Market Rate_US'] = np.log(data['Money Market Rate_US']) # There's typically no compelling reason to logarithmize money market rates.
#data['Money Market Rate_UK'] = np.log(data['Money Market Rate_UK']) # It is common practice not to logarithmize Money market rates. They don't exhibit multiplicative growth patterns in their natural form. My supervisor also suggested me not to logarithmize the interest rate.
data['IPI_US'] = np.log(data['IPI_US'])
data['IPI_UK'] = np.log(data['IPI_UK'])
data['M2_US'] = np.log(data['M2_US'])
data['M2_UK'] = np.log(data['M2_UK'])
data['Stock Market News'] = np.log(data['Stock Market News'])
data['Economic Development News'] = np.log(data['Economic Development News'])
data['FED News'] = np.log(data['FED News'])
data['Micro Finance News'] = np.log(data['Micro Finance News']) #we call this "microeconomic news"
data['International Trade News'] = np.log(data['International Trade News'])
data['EPU_US'] = np.log(data['EPU_US'])
data['EPU_UK'] = np.log(data['EPU_UK'])

# First Differencing
#data['USDGBP'] = np.log(data['USDGBP'])
data = data.diff()
data

#data = data.iloc[2:]
data = data.iloc[1:]
data

# Detect outliers and replace by nan
outliers = []
def detect_outliers_iqr(data):
  for column in data:
    q1 = data[column].quantile(0.25)
    q3 = data[column].quantile(0.75)
    #print(q1, q3)
    IQR = q3-q1
    lwr_bound = q1-(1.5*IQR)
    upr_bound = q3+(1.5*IQR)
    data[column] = data[column].mask(data[column] < q1 - 1.5 * IQR)
    data[column] = data[column].mask(data[column] > q3 + 1.5 * IQR)
detect_outliers_iqr(data)
#print(data)
print(data.isnull().sum()) # print how many outliers we have

# Impute Outliers
for column in data:
  #mean = data[column].mean()
  #data[column].fillna(mean, inplace=True)
  median = data[column].median()
  data[column].fillna(median, inplace=True)
  #print(mean)
print(data.isnull().sum())

# Check if time series are stationary after first differencing
from statsmodels.tsa.stattools import adfuller
for column in data:
  print(column)
  print(adfuller(data[column])[1])
  #P value is always <0.1-> stationary

# First Differencing
#data["Economic Development News"] = data["Economic Development News"].diff()
data

#data = data.iloc[1:]

# final data
data

data.index

# LOG ENTROPIES
# from google.colab import files
# # Define the topics and their corresponding DataFrame columns
# topiccolors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]

# topics = {
#     "Stock Market": "Stock Market News",
#     "Economic Development": "Economic Development News",
#     "ECB": "FED News",
#     "Microeconomics": "Micro Finance News",
#     "International Trade": "International Trade News"
# }

# # Create a plot for each topic
# for i, (topic, column) in enumerate(topics.items(), start=1):
#     fig, ax = plt.subplots(figsize=(11.69, 8.27), dpi=600)  # A4 size in inches
#     ax.plot(data.index, data[column], color=topiccolors[i])  # Use index for time

#     plt.gcf().autofmt_xdate()  # Format x-axis dates
#     plt.xlabel('Time', fontsize=16)
#     plt.ylabel(f'Monthly Entropy to the Topic \n {topic}', fontsize=16)
#     plt.title(f'Monthly Attention to the Topic {i}\n {topic}', fontsize=18)

#     plt.tight_layout()  # Adjust layout to fit labels
#     plt.savefig(f'Monthly_Entropy_to_the_Topic_{topic.replace(" ", "_")}.pdf')  # Save as PDF
#     plt.show()
#     files.download(f'Monthly_Entropy_to_the_Topic_{topic.replace(" ", "_")}.pdf')
#     plt.close()  # Close the plot to free memory

"""# GARCH"""

!pip install arch
from arch import arch_model
Garch = arch_model(data['GBPUSD'], p=1, q=1, mean='constant', vol='GARCH', dist='normal')
gm_result = Garch.fit()
gm_result.summary()
#multivariate BEKK, DCC-GARCH

#Log-Likelihood of VAR model is much higher

import numpy as np
from arch import arch_model

# Assume 'data' is a DataFrame containing EUR/USD returns
train_size = int(len(data) * 0.8)  # Use 80% for training
train_data = data['GBPUSD'][:train_size]
test_data = data['GBPUSD'][train_size:]

# Fit GARCH model to training data
Garch = arch_model(train_data, p=1, q=1, mean='constant', vol='GARCH', dist='normal')
gm_result = Garch.fit(disp='off')

# Forecast for the out-of-sample period
forecast_horizon = len(test_data)
gm_forecast = gm_result.forecast(horizon=forecast_horizon)
forecast_variance = gm_forecast.variance.iloc[-forecast_horizon:].values.flatten()
forecast_mean = gm_forecast.mean.iloc[-forecast_horizon:].values.flatten()

# Calculate RMSFE
actual_values = test_data.values  # True returns
forecast_errors = actual_values - forecast_mean  # Residuals
rmsfe = np.sqrt(np.mean(forecast_errors ** 2))

print(f"RMSFE: {rmsfe}")

# Normalize RMSFE by the standard deviation of the actual values
std_dev_actual = np.std(actual_values)
normalized_rmsfe = rmsfe / std_dev_actual
normalized_rmsfe

# Plotting
plt.figure(figsize=(11.69, 8.27), dpi=600)  # A4 size (landscape) at high DPI
plt.plot(actual_values, label='Actual Values', color='#00A3A1', linestyle='--', marker='o', markersize=5)
plt.plot(forecast_mean, label='Forecasted Values', color='#BC204B', linestyle='-', marker='x', markersize=7)

# Add title with RMSE and Normalized RMSE
plt.title(f"Actual vs Forecasted Values for EUR/USD (RMSE: {rmsfe:.4f}, Normalized RMSE: {normalized_rmsfe:.4f})", fontsize=16)
plt.xlabel('Time Period', fontsize=14)
plt.ylabel('GBP/USD Returns', fontsize=14)

# 95% confidence interval example (if available, here just a placeholder)
# You can also plot the confidence intervals for GARCH forecasts if you have them available
# plt.fill_between(range(len(actual_values)), lower_ci, upper_ci, color='lightgrey', alpha=0.4, label="95% Confidence Interval")

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, len(actual_values), step=2))  # Adjust step size as needed
plt.yticks(fontsize=12)

# Adding grid, legend, and showing the plot
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()  # To ensure everything fits nicely
plt.show()

"""# Correlation Heatmaps"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files

# Load data
# Assuming you have a DataFrame called df
# df = pd.read_csv("your_data.csv")

correlation_matrix = data.corr()

# Visualize heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap GBP/USD')
#plt.savefig('correlation_matrix.png')  # Save heatmap image
plt.savefig('correlation_matrix_GBP.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("correlation_matrix_GBP.pdf")

# import pandas as pd
# import numpy as np
# import statsmodels.api as sm
# from numpy.linalg import eigvals

# # 1. Check the Correlation Matrix
# correlation_matrix = data.corr()
# print("Correlation Matrix:")
# print(correlation_matrix)

# # Highlight correlations above a threshold (e.g., > 0.7)
# high_corr_pairs = [(i, j, correlation_matrix.loc[i, j])
#                    for i in correlation_matrix.columns
#                    for j in correlation_matrix.columns
#                    if i != j and abs(correlation_matrix.loc[i, j]) > 0.7]
# print("\nHigh Correlation Pairs (> 0.7):")
# print(high_corr_pairs)

# # 2. Calculate Variance Inflation Factor (VIF)
# # VIF helps detect multicollinearity in independent variables
# def calculate_vif(data):
#     vif = pd.DataFrame()
#     vif["Variable"] = data.columns
#     vif["VIF"] = [1 / (1 - sm.OLS(data[col], data.drop(columns=[col])).fit().rsquared) for col in data.columns]
#     return vif

# print("\nVariance Inflation Factors (VIF):")
# print(calculate_vif(data))

# # 3. Condition Number (Eigenvalue Analysis)
# # The condition number checks for redundancy in the regression matrix
# X = sm.add_constant(data)  # Add constant for regression matrix
# eigenvalues = eigvals(X.T @ X)  # Eigenvalues of the covariance matrix
# condition_number = np.sqrt(max(eigenvalues) / min(eigenvalues))
# print(f"\nCondition Number: {condition_number}")

# if condition_number > 30:
#     print("Condition number is high (>30), indicating potential multicollinearity.")
# else:
#     print("Condition number is within acceptable range.")

# # 4. Pairwise Plot for Visual Diagnostics (Optional)
# import seaborn as sns
# import matplotlib.pyplot as plt

# sns.pairplot(data)
# plt.title("Pairwise Plot of Variables")
# plt.show()


# When VIF is high due to intrinsic relationships of the economic variables, individual coefficient estimates are less precise.Though, VAR model stability guarantees. If VAR system is stable, it suggests that the dynamics are well-represented and the model can produce reliable forecasts and impulse responses.

determinant = np.linalg.det(correlation_matrix)
print(f"Determinant of the Correlation Matrix: {determinant}")

if determinant < 1e-5:
    print("Low determinant value suggests multicollinearity.")
else:
    print("Determinant is acceptable, multicollinearity is less likely.")

eigenvalues = np.linalg.eigvals(correlation_matrix)
print("Eigenvalues of the Correlation Matrix:")
print(eigenvalues)

if min(eigenvalues) < 1e-5:
    print("Small eigenvalues detected, indicating multicollinearity.")
else:
    print("Small eigenvalues not detected, multicollinearity is less likely.")

"""In VAR modeling the focus is typically on the stability of the system, Granger causality, or Impulse Response Functions (IRFs).

# Robustness test for my news variables
"""

# Robustness test for my news variables
import pandas as pd
import matplotlib.pyplot as plt

# Define your entropy variables
entropy_variables = ['Stock Market News', 'Economic Development News', 'FED News',
                     'Micro Finance News', 'International Trade News', 'EPU_US', 'EPU_UK']

# Define colors for plotting
colors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]

# Iterate over each entropy variable and its corresponding color
for variable, color in zip(entropy_variables, colors):
    # Calculate the rolling mean with a window size of 30 (adjust as needed)
    rolling_mean = data[variable].rolling(window=30).mean()

    # Plot the original data and the rolling mean
    plt.figure(figsize=(10, 6))
    plt.plot(data.index, data[variable], label='Original Data', color=color)
    plt.plot(data.index, rolling_mean, label='Rolling Mean', color='black', linestyle='--')
    plt.title(f'Rolling Mean of {variable}')
    plt.xlabel('Date')
    plt.ylabel(f"Log {variable}")  # Assuming data is logarithmically transformed
    plt.legend()
    plt.grid(True)
    plt.show()

import numpy as np
import matplotlib.pyplot as plt

def robustness_test_function(data, window_size):
    rolling_std = data.rolling(window=window_size).std()
    average_rolling_std = rolling_std.mean()
    print(average_rolling_std)
    threshold = 0.1  # Adjust as needed
    not_robust_vars = data.columns[average_rolling_std > threshold].tolist()
    if not_robust_vars:
        message = f"Warning: {', '.join(not_robust_vars)} may not be robust."
    else:
        message = "All entropy variables appear to be robust."
    return message, not_robust_vars, average_rolling_std

entropy_variables = ['Stock Market News', 'Economic Development News', 'FED News',
                     'Micro Finance News', 'International Trade News', 'EPU_US', 'EPU_UK']

colors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]

results = {}

# Initialize a dictionary to store average rolling standard deviation values
avg_std_dict = {}

for variable, color in zip(entropy_variables, colors):
    message, not_robust_vars, avg_std = robustness_test_function(data[[variable]], window_size=30)
    results[variable] = {"message": message, "not_robust_vars": not_robust_vars}

    # Save the average rolling standard deviation values to the dictionary
    avg_std_dict[variable] = avg_std

    # Print the result including average rolling standard deviation
    print(f"{variable}: {message}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Define your entropy variables
entropy_variables = ['Stock Market News', 'Economic Development News', 'FED News',
                     'Micro Finance News', 'International Trade News']
                     #'EPU_US', 'EPU_UK'

# Define colors for plotting
colors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]
plt.figure(figsize=(6, 4))
# Plot rolling standard deviation for each entropy variable
plt.figure(figsize=(10, 6))  # Adjust figure size as needed

for i, variable in enumerate(entropy_variables):
    rolling_std = data[variable].rolling(window=30).std()
    plt.plot(rolling_std, label=variable, color=colors[i % len(colors)])
# Add y=0.1 line
plt.axhline(y=0.1, color='red', linestyle='--', label='Threshold')
plt.grid(color='grey', linestyle='--', linewidth=0.5)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Rolling Standard Deviation', fontsize=14)
plt.title('Rolling Standard Deviation \nfor Entropy Variables of Topics Related to the U.S. Dollar', fontsize=16)
plt.legend(fontsize=14, frameon=False)
# Adjust layout to accommodate labels
plt.tight_layout()
# Specify the output path
output_path = '/content/drive/MyDrive/topic_plots/Robustness_of_Entropy_variables.pdf'
# Save the plot as a PDF
plt.savefig(output_path, dpi=300)
# Save the plot as a PDF
plt.savefig('plot.pdf')
# Show the plot
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Define your entropy variables
entropy_variables = ['Stock Market News', 'Economic Development News', 'FED News',
                     'Micro Finance News', 'International Trade News', 'EPU_US', 'EPU_UK']

# Define colors for plotting
colors = ["#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333"]

# Plot mean value for each entropy variable
plt.figure(figsize=(10, 6))  # Adjust figure size as needed

for i, variable in enumerate(entropy_variables):
    # Calculate mean value of the time series
    mean_value = data[variable].mean()

    # Plot mean value as a marker with the same color
    plt.scatter([i], mean_value, color=colors[i % len(colors)], label=f'{variable} mean', marker='x')

    # Add label with rounded-up value
    plt.text(i, mean_value, f'{mean_value:.4f}', ha='center', va='bottom')

# Add y=0.1 line
plt.axhline(y=0.1, color='red', linestyle='--', label='Threshold')

plt.xticks(np.arange(len(entropy_variables)), entropy_variables, rotation=45)  # Rotate x-axis labels by 45 degrees
plt.xlabel('Entropy Variables')
plt.ylabel('Average rolling standard deviations')
plt.title('Average rolling standard deviations for News Variables')
plt.legend()
plt.show()

import openpyxl

data.to_excel("Final_Data_GBPUSD_EPU.xlsx")
data.to_excel('/content/drive/MyDrive/Final_Data_GBPUSD_EPU.xlsx', index=False)

"""# Descriptive Statistics of final, cleaned variables"""

data.describe()

"""# Johansen test (just for having a look)"""

data= pd.read_excel('/content/drive/MyDrive/Final_Data_GBPUSD_EPU.xlsx')

import numpy as np
import pandas as pd
from statsmodels.tsa.stattools import coint
result_coint = coint(data['GBPUSD'], data['Stock Market News'])
result_coint = coint(data['GBPUSD'], data['Economic Development News'])
result_coint = coint(data['GBPUSD'], data['FED News'])
result_coint = coint(data['GBPUSD'], data['M2_US'])
result_coint = coint(data['GBPUSD'], data['EPU_US'])
result_coint = coint(data['GBPUSD'], data['EPU_UK'])
# Extract the p-value from the test result
p_value = result_coint[1]
print(p_value)

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.vector_ar.vecm import coint_johansen

# Assuming 'data' is your DataFrame containing the time series data

# Step 1: Perform Johansen cointegration test
def johansen_cointegration_test(data):
    result = coint_johansen(data, det_order=0, k_ar_diff=1)
    print("Eigenvalues:", result.eig)
    print("Trace Statistic:", result.lr1)
    print("Critical Values (90%, 95%, 99%):", result.cvt)

    # Get the number of cointegrating vectors (rank) using maximum eigenvalue test
    rank_max_eig = result.ind
    print("Rank (using max eigenvalue test):", rank_max_eig)

    # Get the number of cointegrating vectors (rank) using trace test
    rank_trace = result.lr2
    print("Rank (using trace test):", rank_trace)

    return result

johansen_result = johansen_cointegration_test(data)

# Step 2: Compute residuals
def compute_residuals(data, johansen_result):
    # Get the cointegrating vectors from the result and reshape if necessary
    cointegrating_vectors = johansen_result.evec
    if len(cointegrating_vectors.shape) == 1:
        cointegrating_vectors = cointegrating_vectors.reshape(-1, 1)
    #print(cointegrating_vectors)
    # Select the relevant cointegrating vectors based on the number of dimensions
    cointegrating_vectors = cointegrating_vectors[:, :16] #johansen_result.ind.max()

    # Project the data onto the cointegrating space
    residuals = data.values - np.dot(data.values, cointegrating_vectors)

    return pd.DataFrame(residuals, index=data.index, columns=data.columns)

residuals = compute_residuals(data, johansen_result)


# Step 3: Check residual stationarity
def check_residual_stationarity(residuals):
    adf_results = {}
    for column in residuals.columns:
        adf_result = sm.tsa.adfuller(residuals[column])
        adf_results[column] = adf_result

    return adf_results

adf_results = check_residual_stationarity(residuals)

# Step 4: Print ADF test results
for column, result in adf_results.items():
    print(f"ADF test results for {column}:")
    print("ADF Statistic:", result[0])
    print("P-value:", result[1])
    print("Critical Values:", result[4])
    print("Is stationary:", result[0] < result[4]['5%'])  # Assuming 5% significance level
    print()

#T-Y wald test

import numpy as np
import statsmodels.api as sm

# Assuming 'data' is your DataFrame
dependent_variable = data.iloc[:, 0]  # Extract dependent variable (first column)
independent_variables = data.iloc[:, 1:16]  # Extract independent variables (remaining columns)

# Fit the multivariate OLS model
model = sm.OLS(dependent_variable, independent_variables).fit()

# Define the null hypothesis matrix 'R' and test whether the coefficients sum to zero
R = np.eye(len(model.params))  # Assuming you want to test each coefficient separately
wald_test = model.wald_test(R)

# Print the Wald test results
print(wald_test)
# very low p-value. These results suggest that there is strong evidence to reject the null hypothesis, indicating that the coefficients of the variables in the model are jointly not equal to zero. In other words, there is statistical evidence to support the presence of at least one significant relationship among the variables included in the model.

# #we do not need this, as we do not go for VECM model

# import numpy as np
# import pandas as pd
# from statsmodels.tsa.vector_ar.vecm import coint_johansen
# from statsmodels.tsa.vector_ar.vecm import VECM
# from statsmodels.tsa.vector_ar.vecm import select_order

# # Assuming you have a DataFrame 'data' containing your variables

# # Step 1: Johansen cointegration test
# johansen_results = coint_johansen(data, det_order=0, k_ar_diff=1)

# # Extract the number of cointegrating vectors
# num_coint_vectors = np.linalg.matrix_rank(johansen_results.eig)

# lag_order = select_order(data, maxlags=10)

# # Step 2: Estimate VECM
# model = VECM(data, k_ar_diff=6) #lag_order
# vecm_results = model.fit()

# # Get impulse response functions
# irf = vecm_results.irf(10)  # Adjust the horizon (10 in this example)

# # Extract cointegrating vectors
# coint_vectors = vecm_results.alpha

# # Construct VAR representation
# A = vecm_results.alpha
# B = vecm_results.beta
# var_coefficients = np.dot(np.linalg.inv(np.eye(data.shape[1]) - A - B), coint_vectors)

# # Create VAR model
# model_var = VAR(data)
# result_var = model_var.fit(var_coefficients, maxlags=6, ic='aic')

# # Compute FEVD for VAR model
# fevd_var = result_var.fevd(10)  # Adjust the horizon (10 in this example)

# # Print or use the results as needed
# print(fevd_var.summary())
# irf = vecm_results.irf(10)
# irf.plot(0)

"""# VAR model"""

#SVAR
#n_vars = 16
#A = np.eye(n_vars)
#A
#svar_model = sm.tsa.SVAR(data, A = A, svar_type='A')
#results = svar_model.fit(maxlags=7)  # Example with maximum lag order of 2
#results.summary()

# Check if time series are normal after first differencing
from statsmodels.tsa.stattools import adfuller
for column in data:
  print(column)
  print(adfuller(data[column])[1])
  #P value is always <0.1-> stationary

# import pandas as pd
# data.to_excel("output.xlsx", sheet_name = 'RadyGBPUSD')
# #data.to_csv('output.csv', index = False)
# data.to_csv('GBPUSD_data.csv')
# #data
# with open('GBPUSD_data.csv', 'w') as f:
#   data.to_csv(f)
# data.to_excel('GBPUSD_data.xlsx', index=False)
# data.to_pickle("GBPUSD.pkl")

# Commented out IPython magic to ensure Python compatibility.
# libraries for VAR
# %matplotlib inline
import numpy as np
import pandas as pd
import scipy
from datetime import datetime, timedelta
import statsmodels.api as sm
from statsmodels.tsa.api import VAR
import matplotlib.pyplot as plt
import pandas_datareader as pdr

#VAR Model lag selection
model = VAR(data) # data[:-1]
ms = model.select_order(7)
ms.summary()

# Fit VAR with 6 lags and see the results
results  = model.fit(7)#maxlags=10, ic='aic'
#results.summary()

"""# Check VAR model"""

log_likelihood = results.llf  # Access the Log-Likelihood value
print(f"Log-Likelihood: {log_likelihood}")

# Normality test (are residuals normally distributed?)
nt = results.test_normality()
nt.summary()
# yes :) they are :)

##Since the p-value (0.937) is much higher than the typical significance level of 0.05, we fail to reject the null hypothesis (Hâ‚€) that the data is normally distributed. #goot from 5 on.

#results.summary()

# checking autocorrelation in residuals
from statsmodels.stats.stattools import durbin_watson
out = durbin_watson(results.resid)
print(out)
# no autocorrelation, dw statistics are always nearly 2.

residuals = results.resid
residuals["GBPUSD"].plot()
residuals["GBPUSD"].mean()
#residuals["GBPUSD"].var() #-8.3419732825863e-12 -1.2325951861483965e-11 9.006365659386069e-12
#5.700218993989929e-05 5.182073259781298e-05 4.5396751666659766e-05

residuals["GBPUSD"].var()

#residuals
#quarterly sums of USDGBP residuals are all 0!!

#results.resid
#results.resid_acorr(1)
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(results.resid["GBPUSD"], lags=24)
print("ACF")

#results.plot()

#results.plot_acorr()

# # Forecasting. RMSE
# forecast_steps = 12  # Define how many steps to forecast ahead
# forecast = results.forecast(data.values[-results.k_ar:], steps=forecast_steps)

# # Create a DataFrame for predictions
# forecast_index = pd.date_range(start=data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')
# predictions = pd.DataFrame(forecast, index=forecast_index, columns=data.columns)

# # Calculate RMSE for actual vs predicted
# actuals = data['GBPUSD'][-forecast_steps:]
# rmse = np.sqrt(mean_squared_error(actuals, predictions['GBPUSD']))
# print(f"RMSE: {rmse:.4f}")

# # Plot actual vs predicted values
# plt.figure(figsize=(14, 7))
# plt.plot(data.index, data['GBPUSD'], label='Actual GBPUSD', color='blue', linewidth=2)
# plt.plot(predictions.index, predictions['GBPUSD'], label='Predicted GBPUSD', color='red', linestyle='--', linewidth=2)
# plt.title(f'Actual vs Predicted GBPUSD Exchange Rate (RMSE: {rmse:.4f})', fontsize=16)
# plt.xlabel('Date', fontsize=14)
# plt.ylabel('GBPUSD Exchange Rate', fontsize=14)
# plt.legend()
# plt.grid(True)  # Add grid for better visualization
# plt.tight_layout()  # Adjust layout
# plt.show()

# #Percentage RMSE

# import numpy as np

# # Convert list to numpy array for easier calculations
# actual_values_array = np.array(data['GBPUSD'])

# # Calculate the mean
# mean_actual = np.mean(actual_values_array)
# mean_actual

# (0.0157/mean_actual)*100

# Calculate R-squared for each equation in the VAR model
for i, column in enumerate(data.columns):
    actual = data[column]
    fitted = results.fittedvalues[column]

    # Calculate Total Sum of Squares (TSS) and Residual Sum of Squares (RSS)
    tss = ((actual - actual.mean()) ** 2).sum()
    rss = ((actual - fitted) ** 2).sum()

    # Calculate R-squared
    rsquared = 1 - (rss / tss)

    print(f'R-squared for {column}: {rsquared:.4f}')

"""# Expanding Window VAR"""

# Testing Residual Correlation
# results  = model.fit(7)#maxlags=10, ic='aic'
# import scipy.stats as stats
# for i in range(results.resid_corr.shape[0]):
#     for j in range(i + 1, results.resid_corr.shape[1]):
#         corr = results.resid_corr[i, j]
#         t_stat = corr * np.sqrt((len(results.resid) - 2) / (1 - corr**2))
#         p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df=len(results.resid) - 2))
#         if p_value < 0.05:
#             print(f"Residuals {i} and {j} are significantly correlated (p={p_value:.3f})")
# #If p-value < 0.05 for any correlation:
# #Adjust the model (e.g., by changing lags or adding variables).
# #Move to an SVAR for deeper analysis if needed.

#rolling window
import numpy as np
import pandas as pd
from statsmodels.tsa.api import VAR
from statsmodels.stats.diagnostic import acorr_ljungbox

# Rolling Window Approach
def rolling_window_var(data, window_size, param_stability_threshold, forecast_error_threshold):
    num_obs, num_vars = data.shape
    robustness_results = []

    # Ensure window size is at least 8
    if window_size < 8:
        raise ValueError("Window size must be at least 8 observations.")

    # Iterate through the data with a rolling window
    for start in range(num_obs - window_size):
        end = start + window_size
        window_data = data[start:end]

        # Dynamically set maxlags based on the window size, ensuring it doesn't exceed window_size - 1
        maxlags = min(7, window_size - 1)  # Ensure lags are less than the number of observations

        # Fit the VAR model on the current window
        model = VAR(window_data)
        model_fitted = model.fit(maxlags=7)  # Fit VAR with lag selection

        # Get forecast for the next period (1 step ahead)
        forecast = model_fitted.forecast(window_data[-model_fitted.k_ar:], steps=1)

        # Get actual data for the next period
        # Ensure that end is within bounds
        if end + 1 < num_obs:
            actual = data[end + 1, :]  # Actual value for the next time step
        else:
            break  # Exit the loop if there's no actual data for the next period

        # Calculate forecast error
        forecast_error = np.abs(forecast - actual)

        # Check if any forecast error exceeds the threshold
        if np.any(forecast_error >= forecast_error_threshold):
            robustness_results.append((start, "Forecast Error: Unstable"))
            continue

        # Check Ljung-Box residuals for autocorrelation
        residuals = model_fitted.resid
        ljungbox_results = []

        # Apply Ljung-Box test for each residual series (variable)
        for i in range(residuals.shape[1]):  # Loop through each variable's residuals
            lb_result = acorr_ljungbox(residuals[:, i], lags=[7], return_df=True)
            p_values = lb_result['lb_pvalue']

            if any(p_value <= 0.05 for p_value in p_values):
                ljungbox_results.append(f"Residuals for variable {i}: Unstable due to autocorrelation")
            else:
                ljungbox_results.append(f"Residuals for variable {i}: Stable")

        # Append Ljung-Box results
        robustness_results.append((start, ljungbox_results))

        # Check parameter stability (variance of parameters)
        param_variance = np.var(model_fitted.params, axis=0)
        if np.any(param_variance >= param_stability_threshold):
            robustness_results.append((start, "Model Parameters: Unstable due to high variance"))
            continue

        # If all checks pass, model is considered robust for this window
        robustness_results.append((start, "The VAR Model is robust"))

    return robustness_results


# Example usage:
data = np.random.rand(188, 16)  # 188 rows x 16 columns example data, replace with your actual data
window_size = 40
param_stability_threshold = 0.05
forecast_error_threshold = 1.2

rolling_results = rolling_window_var(data, window_size, param_stability_threshold, forecast_error_threshold)

# Display the results
for result in rolling_results:
    print(f"Window starting at month {result[0]}: {result[1]}")

# # Define the initial window size
# initial_window = 40  # Adjust based on the size of your dataset

# # Loop through expanding windows
# for end in range(initial_window, len(data)):
#     # Select the expanding subset of data
#     subset = data[:end]

#     # Fit the VAR model
#     model = VAR(subset)
# log_likelihood = results.llf  # Access the Log-Likelihood value
# print(f"Log-Likelihood: {log_likelihood}")
# # Normality test (are residuals normally distributed?)
# nt = results.test_normality()
# nt.summary()
# # yes :) they are :)
# # checking autocorrelation in residuals
# from statsmodels.stats.stattools import durbin_watson
# out = durbin_watson(results.resid)
# print(out)
# # no autocorrelation, dw statistics are always nearly 2.
# #results.resid
# #results.resid_acorr(1)
# from statsmodels.graphics.tsaplots import plot_acf
# plot_acf(results.resid["GBPUSD"], lags=24)

"""# VAR model misspecification"""

#RESET TEST

#High orders needed? No!

import statsmodels.api as sm
import numpy as np
from statsmodels.stats.outliers_influence import reset_ramsey

# Assuming results is your fitted VAR model
fitted_values = results.fittedvalues
residuals = results.resid

# Align data and residuals (ensure lengths match)
data_aligned = data.iloc[7:]  # Adjust based on the number of lags used (7 in your case)

# Handle each equation in the VAR separately
for i in range(fitted_values.shape[1]):
    # Access the i-th column of fitted values (one equation at a time)
    fitted_column = fitted_values.iloc[:, i]  # Corrected: using iloc for DataFrame access

    # Create polynomial terms (squared and cubic) of the fitted values
    X = np.column_stack([fitted_column, fitted_column**2, fitted_column**3])
    X = sm.add_constant(X)  # Add a constant term

    # Fit OLS regression for each equation separately
    reset_model = sm.OLS(data_aligned.iloc[:, i], X).fit()

    # Print the model summary
    print(f"RESET Test for Equation {i + 1}")
    print(reset_model.summary())

    # Apply the Ramsey RESET test on this OLS model
    reset_test = reset_ramsey(reset_model)
    print(reset_test)

#LINK TEST

# Step 2: Generate fitted values and residuals
# fitted_values = results.fittedvalues
# residuals = results.resid

# Reshape to match the structure of residuals and fitted values
fitted_values = pd.DataFrame(fitted_values, index=data.index)  # Ensure the correct index
fitted_values_sq = pd.DataFrame(fitted_values_sq, index=data.index)  # Ensure the correct index

# Step 4: Combine the fitted values and their squared terms
X = pd.concat([fitted_values[7:], fitted_values_sq[7:]], axis=1)
X = sm.add_constant(X)  # Add constant to the regression

# Step 5: Run the regression for each equation
results_LINK = []
for col in residuals.columns:
    y = residuals[col]
    link_model = sm.OLS(y, X).fit()
    results_LINK.append(link_model)

# Step 6: Print the summary of the first equation (you can print others as needed)
print(results_LINK[0].summary())

"""# Out of Sample"""



"""We perform an out-of-sample evaluation of a Vector Autoregressive (VAR) model to assess its predictive accuracy on GBP/USD exchange rates. After splitting the dataset into training and test periods, we estimate the model on the training data to capture the relationships between variables. We then generate forecasts for the test period and compare these forecasts with actual values using the Root Mean Squared Error (RMSE). The resulting Normalized RMSE for GBP/USD is 1.09, indicating that the model's prediction errors are not excessively large relative to the data's variability. This result supports the effectiveness and practical applicability of our approach for forecasting exchange rate movements based on news sentiment, confirming that the model generalizes well to new data."""

# import pandas as pd
# import numpy as np
# from statsmodels.tsa.api import VAR
# from sklearn.metrics import mean_squared_error

# # Example: Assume 'data' is a pandas DataFrame with time series data
# # Split data into training and testing periods
# train_size = int(len(data) * 0.8)  # Use 80% for training
# train_data = data[:train_size]
# test_data = data[train_size:]

# # Check data split
# print(train_data.tail())
# print(test_data.head())

# # Fit the VAR model on training data
# model = VAR(train_data)
# #model_fitted = model.fit(maxlags=8, ic='aic')  # Fit the model with optimal lags
# model_fitted  = model.fit(7)#maxlags=10, ic='aic'

# # Summary of the model
# #print(model_fitted.summary())



# #Out-of-Sample Prediction
# # Forecasting on test data (out-of-sample prediction)
# forecast_steps = len(test_data)  # Forecast the length of the test period
# forecast = model_fitted.forecast(train_data.values[-model_fitted.k_ar:], steps=forecast_steps)

# # Convert forecasted values to a DataFrame for easy comparison
# forecast_df = pd.DataFrame(forecast, columns=train_data.columns, index=test_data.index)

# # Display forecasted vs. actual values
# print(forecast_df.head())
# print(test_data.head())

# # Calculate MSE or RMSE for model evaluation
# mse = mean_squared_error(test_data, forecast_df)
# rmse = np.sqrt(mse)

# print(f'Mean Squared Error: {mse}')
# print(f'Root Mean Squared Error: {rmse}')

#SAMPLE SPLIT 80% 20%

import pandas as pd
import numpy as np
from statsmodels.tsa.api import VAR
from sklearn.metrics import mean_squared_error

# Example: Assume 'data' is a pandas DataFrame with time series data
# Split data into training and testing periods
train_size = int(len(data) * 0.8)  # Use 80% for training
train_data = data[:train_size]
test_data = data[train_size:]

# Check data split
print(train_data.tail())
print(test_data.head())

# Fit the VAR model on training data
model = VAR(train_data)
model_fitted = model.fit(7)  # Fit the model with lag 7

# Out-of-Sample Prediction
# Forecasting on test data (out-of-sample prediction)
forecast_steps = len(test_data)  # Forecast the length of the test period
forecast = model_fitted.forecast(train_data.values[-model_fitted.k_ar:], steps=forecast_steps)

# Convert forecasted values to a DataFrame for easy comparison
forecast_df = pd.DataFrame(forecast, columns=train_data.columns, index=test_data.index)

# Display forecasted vs. actual values
print(forecast_df.head())
print(test_data.head())

# Calculate RMSE for the first equation (first variable in VAR model)
forecasted_values_first_eq = forecast_df.iloc[:, 0].values  # First equation forecast
actual_values_first_eq = test_data.iloc[:, 0].values  # First equation actual values

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(actual_values_first_eq, forecasted_values_first_eq))

# Normalize RMSE by the standard deviation of actual values for the first equation
std_actual_first_eq = np.std(actual_values_first_eq)
normalized_rmse = rmse / std_actual_first_eq

# Output results
print(f"RMSE for the first equation: {rmse}")
print(f"Normalized RMSE for the first equation: {normalized_rmse}")

# Assuming you have your forecasted values and actual values already defined
forecasted_values_first_eq = forecast_df.iloc[:, 0].values  # First equation forecasted
actual_values_first_eq = test_data.iloc[:, 0].values  # First equation actual values

# Plotting
plt.figure(figsize=(11.69, 8.27), dpi=600)  # A4 size (landscape) at high DPI
plt.plot(actual_values_first_eq, label='Actual Values', color='#00A3A1', linestyle='--', marker='o', markersize=5)
plt.plot(forecasted_values_first_eq, label='Forecasted Values', color='#BC204B', linestyle='-', marker='x', markersize=7)

# Add title with RMSE
plt.title(f"Actual vs Forecasted Values for GBP/USD (RMSE: {rmse:.4f}, Normalized RMSE: {normalized_rmse:.4f})", fontsize=16)
plt.xlabel('Time Period', fontsize=14)
plt.ylabel('Values', fontsize=14)

# 95% confidence interval example (if available, here just a placeholder)
# plt.fill_between(range(len(actual_values_first_eq)), lower_ci, upper_ci, color='lightgrey', alpha=0.4, label="95% Confidence Interval")

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, len(actual_values_first_eq), step=2))  # Adjust step size as needed
plt.yticks(fontsize=12)

# Adding grid, legend, and showing the plot
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()  # To ensure everything fits nicely
plt.show()

data_GBP_Out_of_Sample

data_GBP_Out_of_Sample

data_GBP_Out_of_Sample[1]

data_GBP_Out_of_Sample = pd.read_excel('/content/drive/MyDrive/GBP_USD_Out_of_Sample.xlsx', parse_dates =['Date'])

data_GBP_Out_of_Sample.index = data_GBP_Out_of_Sample.Date
data_GBP_Out_of_Sample['GBPUSD'] = np.log(data_GBP_Out_of_Sample['GBPUSD'])
data_GBP_Out_of_Sample=data_GBP_Out_of_Sample.diff()
data_GBP_Out_of_Sample = data_GBP_Out_of_Sample.drop('Date', axis=1)
data_GBP_Out_of_Sample.index = pd.to_datetime(data_GBP_Out_of_Sample.index, format='%m.%d.%Y')

forecast_steps = 48  # Forecast the length of the test period
forecast = results.forecast(data.values[-results.k_ar:], steps=forecast_steps)
forecast_df = pd.DataFrame(forecast, columns=data.columns)

forecasted_values_first_eq=forecast_df.iloc[1:, 0].values
#forecasted_values_first_eq_df= pd.DataFrame(forecasted_values_first_eq)
##forecasted_values_first_eq.index =data_GBP_Out_of_Sample.index[0:48]


forecasted_values_first_eq_df.iloc[1:].index =pd.date_range(start="2017-01-10", periods=47, freq="M")
data_GBP_Out_of_Sample.iloc[1:48].index=pd.date_range(start="2017-01-10", periods=47, freq="M")
data_GBP_Out_of_Sample=data_GBP_Out_of_Sample.iloc[1:48, 0].values
t= pd.date_range(start="2017-01-10", periods=47, freq="M")

data_GBP_Out_of_Sample
# Plotting
plt.figure(figsize=(11.69, 8.27), dpi=600)  # A4 size (landscape) at high DPI
plt.plot(data_GBP_Out_of_Sample, label='Actual Values', color='#00A3A1', linestyle='--', marker='o', markersize=5)
plt.plot(forecasted_values_first_eq, label='Forecasted Values', color='#BC204B', linestyle='-', marker='x', markersize=7)

# Add title with RMSE
plt.title(f"Actual vs Forecasted Values for GBP/USD (RMSFE: {rmse:.4f}, Normalized RMSFE: {normalized_rmse:.4f})", fontsize=16)
plt.xlabel('Time Period', fontsize=14)
plt.ylabel('Values', fontsize=14)

# 95% confidence interval example (if available, here just a placeholder)
# plt.fill_between(range(len(actual_values_first_eq)), lower_ci, upper_ci, color='lightgrey', alpha=0.4, label="95% Confidence Interval")

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, len(actual_values_first_eq), step=2))  # Adjust step size as needed
plt.yticks(fontsize=12)

# Adding grid, legend, and showing the plot
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()  # To ensure everything fits nicely
plt.show()

"""# RMSE Cross-validation"""

import numpy as np
import pandas as pd
from statsmodels.tsa.api import VAR
from sklearn.metrics import mean_squared_error

# Assuming aligned_data is a DataFrame with actual data in column 'Actual'

# Parameters for rolling window
window_size = 24 # Define the window size for training
forecast_steps = 1  # Number of steps to forecast ahead

# Lists to store results
predictions = []
actuals = []
# Perform rolling window predictions
for end in range(window_size, len(data) - forecast_steps + 1):
    # Define the training window
    train_data = data.iloc[end - window_size:end]

    # Fit the VAR model
    model = VAR(train_data)


    results = model.fit(7)  # Choose lag based on prior analysis or model selection criteria

    # Make forecast
    forecast = results.forecast(train_data.values, steps=forecast_steps)

    # Store the predictions and actual values
    predictions.append(forecast[-1, 0])  # Append the last prediction
    actuals.append(data['GBPUSD'].iloc[end + forecast_steps - 1])

# Convert results to DataFrame for analysis
aligned_data = pd.DataFrame({'Predicted': predictions, 'Actual': actuals})

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(aligned_data['Actual'], aligned_data['Predicted']))
print(f"Rolling Window Forecast RMSE: {rmse}")

# # Display results
# print(aligned_data.head())

#Cross validation
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

# Number of splits for time series cross-validation
n_splits = 6
tscv = TimeSeriesSplit(n_splits=n_splits)

# Initialize lists to store RMSE values
train_rmse_scores = []
test_rmse_scores = []

# Perform time series cross-validation
for train_index, test_index in tscv.split(aligned_data):
    # Split data into train and test sets
    X_train, X_test = aligned_data['Actual'][train_index],  aligned_data['Actual'][test_index]
    y_train, y_test = aligned_data['Predicted'][train_index], aligned_data['Predicted'][test_index] #['Predicted_withlags'][

    # Compute RMSE for training set (to check for overfitting)
    train_rmse = np.sqrt(mean_squared_error(y_train, X_train))
    train_rmse_scores.append(train_rmse)

    # Compute RMSE for test set (generalization performance)
    test_rmse = np.sqrt(mean_squared_error(y_test, X_test))
    test_rmse_scores.append(test_rmse)

    print(f"Train RMSE: {train_rmse}")
    print(f"Test RMSE: {test_rmse}")
    print("---")

# Calculate average RMSE across folds
avg_train_rmse = np.mean(train_rmse_scores)
avg_test_rmse = np.mean(test_rmse_scores)

print(f"Average Train RMSE: {avg_train_rmse}")
print(f"Average Test RMSE: {avg_test_rmse}")

# Check for overfitting
if avg_train_rmse < avg_test_rmse:
    print("Potential Overfitting Detected: Average Train RMSE is lower than Average Test RMSE")
else:
    print("No Overfitting Detected: Average Train RMSE is not significantly lower than Average Test RMSE")

#Percentage RMSE

import numpy as np

# Convert list to numpy array for easier calculations
actual_values_array = np.array(aligned_data['Actual'])

# Calculate the mean
mean_actual = np.mean(actual_values_array)
mean_actual

(0.048/mean_actual)*100

# --------------------- 1. Simple Train/Test Split ---------------------
train_size = int(len(data) * 0.8)  # 80% for training
train, test = data.iloc[:train_size], data.iloc[train_size:]

print("\nTrain/Test Split:")
print("Train Set:\n", train)
print("Test Set:\n", test)

# --------------------- 2. K-Fold Cross-Validation ---------------------
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)  # 5 splits
print("\nK-Fold Cross-Validation Splits:")
for train_index, test_index in tscv.split(data):
    train_cv, test_cv = data.iloc[train_index], data.iloc[test_index]
    print(f"\nTrain Indices: {train_index[-1]} | Test Indices: {test_index[-1]}")
    print("Train Set:\n", train_cv)
    print("Test Set:\n", test_cv)

# --------------------- 3. Rolling Forecast Origin ---------------------
window_size = 12  # Window size for training
print("\nRolling Forecast Origin Splits:")
for end in range(window_size, len(data)):
    train_roll = data.iloc[end - window_size:end]
    test_roll = data.iloc[end:end + 1]  # Predicting the next month
    print(f"\nTrain Window: {train_roll.index[-1]} | Test Month: {test_roll.index[0]}")
    print("Train Set:\n", train_roll)
    print("Test Set:\n", test_roll)

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.api import VAR

# Example: Creating a sample DataFrame with GBPUSD data
# In your case, you should load your actual GBPUSD data
# Replace this section with your actual GBPUSD exchange rate data

# Split the data into train and test sets (80/20 split)
train_size = int(len(data) * 0.8)
train, test = data.iloc[:train_size], data.iloc[train_size:]

# Fit the VAR model on the training data
model = VAR(train)
results = model.fit(maxlags=7)  # Choose the lag order

# Forecasting using the model
forecast = results.forecast(train.values[-results.k_ar:], steps=len(test))
test_predictions = pd.DataFrame(forecast, index=test.index, columns=test.columns)

# # Calculate RMSE for training and test sets
# train_rmse = np.sqrt(mean_squared_error(train['GBPUSD'], results.fittedvalues))
# test_rmse = np.sqrt(mean_squared_error(test['GBPUSD'], test_predictions['GBPUSD']))

# print(f"Train RMSE: {train_rmse:.4f}")
# print(f"Test RMSE: {test_rmse:.4f}")

# # Analysis of Overfitting
# if train_rmse < test_rmse:
#     print("The model may be overfitting the training data.")
# elif train_rmse > test_rmse:
#     print("The model may be underfitting the training data.")
# else:
#     print("The model fits the data well.")

# # Show the actual vs predicted for visual analysis (optional)
# import matplotlib.pyplot as plt

plt.figure(figsize=(14, 7))
plt.plot(data.index, data['GBPUSD'], label='Actual GBPUSD', color='blue')
plt.plot(test_predictions.index, test_predictions['GBPUSD'], label='Predicted GBPUSD', color='red')
plt.title('Actual vs Predicted GBPUSD Exchange Rate')
plt.xlabel('Date')
plt.ylabel('GBPUSD Exchange Rate')
plt.legend()
plt.show()

"""# FEVDs"""

# You can have a look how nicely the numbers in the first column fall... It's so beautiful :) The FEVD plots here are not colorful, so I plot this in R.

# autocorrelation with itself dominates
fevd = results.fevd(49)
#fevd.summary()
fevd_summary = fevd.summary()
fevd_summary

fevd.plot()

#print latex(fevd_summary)

#colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44", "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

fevd.names

#correct logic: [0, :, i]

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'fevd_decomp' is your 3D NumPy array.
# Replace this with your actual 'fevd_decomp' data
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
fevd_decomp

# # Retrieve first element from each row along with its position
# values_with_positions = []
# for block_index in range(fevd_decomp.shape[0]):
#     for row_index in range(fevd_decomp.shape[1]):
#         value = fevd_decomp[block_index, row_index, 0]
#         position = (block_index, row_index, 0)
#         values_with_positions.append((value, position))

# # Display the result
# for value, position in values_with_positions:
#     print(f"Value: {value}, Position: {position}")

#fevd_decomp[0, :, 0]

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'fevd_decomp' is your 3D NumPy array.

fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Create a new array to store the modified data for plotting
modified_elements = np.zeros((num_variables - 6, first_elements_all.shape[1]))  # to account for summing 7

# Copy the data for the first few variables
modified_elements[:num_variables - 7, :] = first_elements_all[:num_variables - 7, :]

# Sum the last 7 variables and store them in the new array
modified_elements[num_variables - 7, :] = np.sum(first_elements_all[-7:, :], axis=0)

# Colors for plotting (you can adjust as needed)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
plt.figure(figsize=(12, 8))
fevdnames= ['GBP/USD', 'Inflation US', 'Inflation UK', 'Interest Rate US', 'Interest Rate UK', 'Economic Activity US', 'Economic Activity UK', 'M2 US', 'M2 UK'] #'Stock Market News', 'Economic Development News', 'FED News', 'Micro Finance News', 'International Trade News', , 'EPU_US', 'EPU_UK'

# Plot each time series
for i in range(modified_elements.shape[0]):
    if i < modified_elements.shape[0] - 1:
        plt.plot(modified_elements[i]*100, label=fevdnames[i], color=colors[i])  # Original labels for the rest #label=fevd.names[i]
    else:
        plt.plot(modified_elements[i]*100, label='All News + EPU', color='yellow')  # Label for the summed series

# Add dotted grid lines
plt.grid(True, which='both', linestyle=':', linewidth=0.5)

# Set x-axis ticks with a step of 2, starting from 1 but plotting 0 in the first period
#x_ticks = np.arange(1, modified_elements.shape[1] + 1, step=2)
#plt.xticks(x_ticks)  # Set the x-ticks
x_ticks = np.arange(0, modified_elements.shape[1] + 1, step=6)
plt.xticks(x_ticks)

# Modify x-ticks to show labels starting from 0
#plt.xticks(np.arange(0, modified_elements.shape[1]), np.arange(1, modified_elements.shape[1] + 1, step=1))



# Set y-axis ticks with a step of 0.1
#plt.yticks(np.arange(0, 1.1, step=0.1))  # Assuming values on y-axis range between 0 and 1
plt.yticks(np.arange(0, 101, step=10))  # Assuming values on y-axis range between 0 and 1


plt.title('FEVD for EUR/USD')
plt.xlabel('Time')
plt.ylabel('Contribution (%) to EUR/USD')
plt.legend()

# Show plot
plt.show()

# import numpy as np
# import matplotlib.pyplot as plt

# # Assuming 'fevd_decomp' is your 3D NumPy array.
# # Replace this with your actual 'fevd_decomp' data
# fevd_decomp = fevd.decomp

# # Extracting the first elements from all inner structures
# first_elements_all = [fevd_decomp[i, :, 0] for i in range(fevd_decomp.shape[0])]

# # Convert to a 2D NumPy array for easier manipulation
# first_elements_all = np.array(first_elements_all)

# # Identifying the number of variables and time points
# num_variables = first_elements_all.shape[0]

# # Create a new array to store the modified data for plotting
# modified_elements = np.zeros((num_variables - 6, first_elements_all.shape[1]))  # to account for summing 7

# # Copy the data for the first few variables
# modified_elements[:num_variables - 7, :] = first_elements_all[:num_variables - 7, :]

# # Sum the last 7 variables and store them in the new array
# modified_elements[num_variables - 7, :] = np.sum(first_elements_all[-7:, :], axis=0)

# # Colors for plotting (you can adjust as needed)
# colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
#           "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
#           "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# # Plotting
# plt.figure(figsize=(12, 8))

# x_values = np.arange(1, modified_elements.shape[1] + 1)

# # Plot each time series
# for i in range(modified_elements.shape[0]):
#     if i < modified_elements.shape[0] - 1:
#         plt.plot(x_values, modified_elements[i]*100, label=fevd.names[i], color=colors[i])  # Original labels for the rest
#     else:
#         plt.plot(x_values, modified_elements[i]*100, label='All News', color='yellow')  # Label for the summed series

# # Add dotted grid lines
# plt.grid(True, which='both', linestyle=':', linewidth=0.5)

# # Set x-axis ticks with a step of 2, starting from 1 but plotting 0 in the first period
# #x_ticks = np.arange(1, modified_elements.shape[1] + 1, step=2)
# #plt.xticks(x_ticks)  # Set the x-ticks
# x_ticks = np.arange(0, modified_elements.shape[1] + 1, step=6)
# plt.xticks(x_ticks)

# # Modify x-ticks to show labels starting from 0
# #plt.xticks(np.arange(0, modified_elements.shape[1]), np.arange(1, modified_elements.shape[1] + 1, step=1))



# # Set y-axis ticks with a step of 0.1
# #plt.yticks(np.arange(0, 1.1, step=0.1))  # Assuming values on y-axis range between 0 and 1
# plt.yticks(np.arange(0, 101, step=10))  # Assuming values on y-axis range between 0 and 1


# plt.title('FEVD for EUR/USD')
# plt.xlabel('Time')
# plt.ylabel('Contribution (%) to EUR/USD')
# plt.legend()

# # Show plot
# plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'fevd_decomp' is your 3D NumPy array.
# Replace this with your actual 'fevd_decomp' data
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Create a new array to store the modified data for plotting
modified_elements = np.zeros((num_variables - 6, first_elements_all.shape[1]))  # to account for summing 7

# Copy the data for the first few variables
modified_elements[:num_variables - 7, :] = first_elements_all[:num_variables - 7, :]

# Sum the last 7 variables and store them in the new array
modified_elements[num_variables - 7, :] = np.sum(first_elements_all[-7:, :], axis=0)

# Colors for plotting (you can adjust as needed)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
plt.figure(figsize=(12, 8))

x_values = np.arange(1, modified_elements.shape[1] + 1)

# Plot each time series
for i in range(modified_elements.shape[0]):
    if i < modified_elements.shape[0] - 1:
        plt.plot(x_values, modified_elements[i]*100, label=fevd.names[i], color=colors[i])  # Original labels for the rest
    else:
        plt.plot(x_values, modified_elements[i]*100, label='All News', color='yellow')  # Label for the summed series

# Add dotted grid lines
plt.grid(True, which='both', linestyle=':', linewidth=0.5)

# Set x-axis ticks with a step of 2, starting from 1 but plotting 0 in the first period
#x_ticks = np.arange(1, modified_elements.shape[1] + 1, step=2)
#plt.xticks(x_ticks)  # Set the x-ticks
x_ticks = np.arange(0, modified_elements.shape[1] + 1, step=6)
plt.xticks(x_ticks)

# Modify x-ticks to show labels starting from 0
#plt.xticks(np.arange(0, modified_elements.shape[1]), np.arange(1, modified_elements.shape[1] + 1, step=1))

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]


# Colors for plotting
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"] #"#EAAA00", "#F68D2E"

# Plotting
plt.figure(figsize=(12, 8))

# Plot each time series as an area graph
for i in range(first_elements_all.shape[0]):
    plt.fill_between(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
                     label=fevd.names[i], color=colors[i % len(colors)], alpha=0.4)  # Use colors cyclically if more than available

plt.title('FEVD for EUR/USD')
plt.xlabel('Time')
plt.ylabel('Contribution (%) to EUR/USD')
plt.legend()
plt.grid(True)
plt.xticks(np.arange(1, first_elements_all.shape[1] + 1, step=2))  # Set x-ticks to start from 1
plt.xlim(1, first_elements_all.shape[1])  # Set x-limits to start from 1
plt.show()

import pandas as pd
#fevdnames_all= ['GBP/USD', 'Inflation US', 'Inflation UK', 'Interest Rate US', 'Interest Rate UK', 'Economic Activity US', 'Economic Activity UK', 'M2 US', 'M2 UK','Stock Market News', 'Economic Development News', 'FED News', 'Microeconomic News', 'International Trade News', 'EPU US', 'EPU UK']
fevdnames_short_all= ['GBP/USD', 'CPI US', 'CPI UK', 'IR US', 'IR UK', 'IPI US', 'IPI UK', 'M2 US', 'M2 UK','Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'EPU US', 'EPU UK']

# Initialize an empty list to store the data
# Initialize an empty list to store the data
news_impact = []

# Loop through the range to gather data, multiply by 100, and round to one decimal place
for i in range(0, 16):
    row = [f"{x.round(1):.1f}" for x in (first_elements_all[i] * 100)]

    #row = list((first_elements_all[i] * 100).round(1))  # assuming first_elements_all has rows for each element
    news_impact.append(row)

# Convert the data to a DataFrame
# 'Month' column will have indices as strings to identify each period
months = [f"{j}" for j in range(first_elements_all.shape[1])]
news_impact_df = pd.DataFrame(news_impact, columns=months, index=fevdnames_all).transpose()
# Set the name of the index column
news_impact_df.index.name = "Month"
news_impact_df = news_impact_df.drop(news_impact_df.index[0])
# Display the table
news_impact_df

news_impact_df

#Save Values to a Latex Table

import pandas as pd
from google.colab import files
# Convert DataFrame to LaTeX format
latex_table = news_impact_df.to_latex(index=True, header=False, column_format="p{0.8cm} *{21}{p{0.35cm}}", escape=False)

# Adding custom headers and LaTeX structure
custom_headers = r"""
\begin{tabular}{p{0.8cm} *{21}{p{0.35cm}}}
\hline
Month & GBP & CPI & CPI & IR & IR & IPI & IPI & M2 & M2 & Topic& Topic& Topic& Topic& Topic& EPU & EPU \\
$\phantom{0}$ & /USD & US & UK & US & UK & US & UK & US & UK & 1 & 2 & 3 & 4 & 5 & US & UK \\
\hline
"""

# Adding headers and LaTeX structure to the table
latex_table = custom_headers + latex_table + r"\end{tabular}"
file_path = '/content/drive/MyDrive//table.tex'
# Print or save to a .tex file
with open(file_path, "w") as file:
    file.write(latex_table)

import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Calculate the sum of the last 7 variables (All News Variables)
sum_all_news = np.sum(first_elements_all[-7:, :], axis=0)

# Calculate the sum of variables 2 through 9 (All Macroeconomic Variables)
sum_macro_variables = np.sum(first_elements_all[1:9, :], axis=0)  # Index 1 to 8 corresponds to variables 2 to 9

# Colors for plotting
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
plt.figure(figsize=(11.69,8.27), dpi=600)

# # Plot each time series as an area graph for individual variables
# for i in range(first_elements_all.shape[0]):
#     plt.fill_between(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
#                      label=fevd.names[i], color=colors[i % len(colors)], alpha=0.3)  # Original variables

# Extract the first variable, the FX
first_variable = first_elements_all[0, :]
plt.fill_between(np.arange(0, first_variable.shape[0] ), first_variable * 100,
                 label=fevd.names[0], color=colors[0], alpha=0.7)

# Plot the sum of All Macroeconomic Variables
plt.fill_between(np.arange(0, sum_macro_variables.shape[0] ), sum_macro_variables * 100,
                 label='All Macroeconomic Variables', color='#FF7F50', alpha=0.7)

# Plot the sum of All News Variables
plt.fill_between(np.arange(0, sum_all_news.shape[0]), sum_all_news * 100,
                 label='All News Variables', color='yellow', alpha=0.7)


plt.title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
plt.legend()
plt.grid(True)
plt.xticks(np.arange(0, first_elements_all.shape[1]+1 , step=2), fontsize=12)  # Set x-ticks to start from 1
plt.xlim(0, first_elements_all.shape[1])  # Set x-limits to start from 1

# Set y-axis ticks to show every 5 units
plt.yticks(np.arange(0, 101, 10), fontsize=12)
plt.savefig('FEVD_for_GBP_USD_Area_Plot.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("FEVD_for_GBP_USD_Area_Plot.pdf")

import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Calculate the sum of the last 7 variables (All News Variables)
sum_all_news = np.sum(first_elements_all[-7:, :], axis=0)

# Calculate the sum of variables 2 through 9 (All Macroeconomic Variables)
sum_macro_variables = np.sum(first_elements_all[1:9, :], axis=0)  # Index 1 to 8 corresponds to variables 2 to 9

# Colors for plotting
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
plt.figure(figsize=(11.69,8.27), dpi=600)


# Normalize the values so they add up to 100% at each time point
stacked_data = np.vstack([first_variable, sum_all_news, sum_macro_variables])
stacked_data_normalized = stacked_data / stacked_data.sum(axis=0)

# Colors for each group
colors = ["#00338D", "yellow", "#FF7F50"]

# Plotting
plt.figure(figsize=(11.69, 8.27), dpi=600)

# Create the stacked area plot with normalized values
plt.stackplot(np.arange(stacked_data_normalized.shape[1]),
              stacked_data_normalized * 100,  # Scale to 100%
              labels=["FX", "All News Variables", "All Macroeconomic Variables"],
              colors=colors, alpha=0.7)


plt.title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
# Create a second y-axis (on the right)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)


plt.legend()
plt.grid(True)
plt.xticks(np.arange(0, first_elements_all.shape[1]+1 , step=2), fontsize=12)  # Set x-ticks to start from 1
plt.xlim(0, first_elements_all.shape[1])  # Set x-limits to start from 1

# Set y-axis ticks to show every 5 units
plt.yticks(np.arange(0, 101, 10), fontsize=12)
plt.savefig('FEVD_for_GBP_USD_Area_Plot.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("FEVD_for_GBP_USD_Area_Plot.pdf")

import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Calculate the sum of the last 7 variables (All News Variables)
sum_all_news = np.sum(first_elements_all[-7:, :], axis=0)

# Calculate the sum of variables 2 through 9 (All Macroeconomic Variables)
sum_macro_variables = np.sum(first_elements_all[1:9, :], axis=0)  # Index 1 to 8 corresponds to variables 2 to 9

# Colors for plotting
colors = ["#00338D", "yellow", "#FF7F50"]

# Normalize the values so they add up to 100% at each time point
stacked_data = np.vstack([first_elements_all[0, :], sum_all_news, sum_macro_variables])  # Use the first variable (FX)
stacked_data_normalized = stacked_data / stacked_data.sum(axis=0)

# Plotting
fig, ax1 = plt.subplots(figsize=(11.69, 8.27), dpi=600)

# Create the stacked area plot with normalized values
ax1.stackplot(np.arange(stacked_data_normalized.shape[1]),
              stacked_data_normalized * 100,  # Scale to 100%
              labels=["FX", "All News Variables", "All Macroeconomic Variables"],
              colors=colors, alpha=0.7)

# Set labels and title
ax1.set_title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
ax1.set_xlabel('Time (Months)', fontsize=16)
ax1.set_ylabel('Contribution (%) to GBP/USD', fontsize=16)

# Set x-ticks and y-ticks for the first axis
ax1.set_xticks(np.arange(0, first_elements_all.shape[1] + 1, step=2))
ax1.set_xticklabels(np.arange(0, first_elements_all.shape[1] + 1, step=2), fontsize=12)
ax1.set_yticks(np.arange(0, 101, 10))
ax1.set_yticklabels(np.arange(0, 101, 10), fontsize=12)

# Create the second y-axis on the right side
ax2 = ax1.twinx()

# Set the label for the second y-axis (you can change this to represent another scale or metric)
ax2.set_ylabel('Alternative Metric (%)', fontsize=16)

# Optionally, you can customize the second y-axis ticks, here we just show it in the same range as the first y-axis
ax2.set_yticks(np.arange(0, 110, 10))  # Adjust as needed
ax2.set_yticklabels(np.arange(0, 110, 10), fontsize=12)

# Adjust plot limits
ax1.set_xlim(0, first_elements_all.shape[1])  # Set x-limits
ax2.set_ylim(0, 100)  # Optionally adjust y-limits for the second axis

# Add the legend for the stacked plot
ax1.legend(loc='upper left')

# Add grid and show plot
ax1.grid(True)

# Save the figure as a PDF
plt.savefig('FEVD_for_GBP_USD_Area_Plot_with_2YAxes.pdf', bbox_inches='tight')

# Show the plot
plt.show()

# Download the plot (optional)
files.download("FEVD_for_GBP_USD_Area_Plot_with_2YAxes.pdf")

import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Calculate the sum of the last 7 variables (All News Variables)
sum_all_news = np.sum(first_elements_all[-7:, :], axis=0)

# Calculate the sum of variables 2 through 9 (All Macroeconomic Variables)
sum_macro_variables = np.sum(first_elements_all[1:9, :], axis=0)  # Index 1 to 8 corresponds to variables 2 to 9

# Colors for plotting
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
plt.figure(figsize=(11.69,8.27), dpi=600)

# # Plot each time series as an area graph for individual variables
# for i in range(first_elements_all.shape[0]):
#     plt.fill_between(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
#                      label=fevd.names[i], color=colors[i % len(colors)], alpha=0.3)  # Original variables

# Extract the first variable, the FX
first_variable = first_elements_all[0, :]

plt.plot(np.arange(first_variable.shape[0]), first_variable * 100, label='GBP/USD', color='#00338D', linewidth=2.5)
plt.plot(np.arange(sum_all_news.shape[0]), sum_all_news * 100, label='All News Variables', color='yellow', linewidth=2.5)
plt.plot(np.arange(sum_macro_variables.shape[0]), sum_macro_variables * 100, label='All Macroeconomic Variables', color='#FF7F50', linewidth=2.5)


plt.title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
plt.legend()
plt.grid(True)
plt.xticks(np.arange(0, first_elements_all.shape[1]+1 , step=2), fontsize=12)  # Set x-ticks to start from 1
plt.xlim(0, first_elements_all.shape[1])  # Set x-limits to start from 1

# Set y-axis ticks to show every 5 units
plt.yticks(np.arange(0, 101, 10), fontsize=12)
plt.savefig('FEVD_for_GBP_USD_Line_Plot.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("FEVD_for_GBP_USD_Line_Plot.pdf")

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]

# Colors for plotting
colors = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]

# Plotting
#plt.figure(figsize=(12, 8))
plt.figure(figsize=(11.69,8.27), dpi=600)
# # Plot the last 7 time series as individual area graphs
# for i in range(-7, 0):
#     plt.fill_between(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
#                      label=fevd.names[i], color=colors[i % len(colors)], alpha=0.3)  # Use colors cyclically if needed

fevdnames_all= ['GBP/USD', 'Inflation US', 'Inflation UK', 'Interest Rate US', 'Interest Rate UK', 'Economic Activity US', 'Economic Activity UK', 'M2 US', 'M2 UK','Stock Market News', 'Economic Development News', 'FED News', 'Microeconomic News', 'International Trade News', 'EPU US', 'EPU UK']

for i in range(-7, 0):
    plt.plot(np.arange(0, first_elements_all.shape[1]), first_elements_all[i] * 100,
                     label=fevdnames_all[i], color=colors[i], linewidth=2.5)  # Use colors cyclically if needed

plt.title('FEVD for GBP/USD: \n Contribution of News-Derived Variables', fontweight='bold', fontsize=20)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
plt.legend(loc ='upper right')
plt.grid(True)
plt.xticks(np.arange(0, first_elements_all.shape[1], step=2), fontsize=12)  # Set x-ticks to start from 1
plt.yticks(fontsize=12)
plt.xlim(0, first_elements_all.shape[1])  # Set x-limits to start from 1
plt.ylim(0, 6.3)  # Set x-limits to start from 1
plt.savefig('GBP_USD_Contribution_of_News-Derived_Variables.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("GBP_USD_Contribution_of_News-Derived_Variables.pdf")

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'fevd_decomp' is your 3D NumPy array.
fevd_decomp = fevd.decomp

# Extracting the first elements from all inner structures
first_elements_all = [fevd_decomp[0, :, i] for i in range(fevd_decomp.shape[0])]

# Convert to a 2D NumPy array for easier manipulation
first_elements_all = np.array(first_elements_all)

# Identifying the number of variables and time points
num_variables = first_elements_all.shape[0]


# Colors for plotting
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
#plt.figure(figsize=(12, 8))
plt.figure(figsize=(11.69,8.27), dpi=600)
# Plot variables 1 to 8 (indices 1 to 8, which corresponds to indices 0 to 7 in Python)
for i in range(1, 9):  # Loop over variables 1 to 8 (indices 1 to 8)
    plt.plot(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
                     label=fevdnames[i], color=colors[i], linewidth=2.5)  # Use colors and labels


plt.title('FEVD for GBP/USD: \n Contribution of Macroeconomic Variables', fontweight='bold', fontsize=20)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
plt.legend()
plt.grid(True)
plt.xticks(np.arange(0, first_elements_all.shape[1], step=2), fontsize=12)  # Set x-ticks to start from 1
plt.yticks(fontsize=12)  # Set x-ticks to start from 1

plt.xlim(0, first_elements_all.shape[1])  # Set x-limits to start from 1
plt.savefig('GBP_USD_Contribution_of_Macroeconomic_Variables.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("GBP_USD_Contribution_of_Macroeconomic_Variables.pdf")

# Display the DataFrame in LaTeX format
#print(data_summary.to_latex())

# Export the DataFrame to a LaTeX file
#df_summary.to_latex("summary.tex")

"""# GIRF"""

# import numpy as np
# import matplotlib.pyplot as plt
# from statsmodels.tsa.api import VAR
# from scipy.linalg import sqrtm

# # Assuming `results` is your fitted VAR model
# n_steps = 10  # Number of steps for the GIRF
# n_vars = results.neqs  # Number of variables in the VAR model

# # Step 1: Extract residual covariance matrix (Î£_Îµ) and calculate square root
# cov_matrix = results.sigma_u.values if hasattr(results.sigma_u, 'values') else results.sigma_u  # Convert to NumPy array if needed
# sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ

# # Step 2: Get the MA coefficient matrices A_n for each time step
# ma_coeffs = results.ma_rep(n_steps)  # MA coefficient matrices A_n up to n_steps

# # Initialize array to store GIRFs (time steps x variables), only for the specified shock variable
# girfs = np.zeros((n_steps, n_vars))

# # Define the specific shock variable (10th variable, so index 9 in Python)
# shock_var = 9  # Index for the 10th variable

# # Step 3: Calculate GIRF for the 10th variable
# sigma_ii = cov_matrix[shock_var, shock_var]  # The diagonal element Ïƒ_ii for the shock_var
# scaling_factor = 1 / np.sqrt(sigma_ii)  # Calculate Ïƒ_ii^(-1/2)

# # For each time step, compute the GIRF using the formula
# for step in range(n_steps):
#     # A_n @ Î£_Îµ @ e_i, where e_i is a selection vector for the shock variable
#     response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
#     girfs[step, :] = response  # Store the response for each variable at this time step

# # Step 4: Plot the GIRFs for the response of each variable to the shock in the 10th variable
# plt.figure(figsize=(12, 6))
# for i in range(n_vars):
#     plt.plot(girfs[:, i], label=f'GIRF for variable {fevd.names[i]} in response to shock in {fevd.names[shock_var]}')
# plt.title(f"Generalized Impulse Response Functions (GIRFs) for Shock in {fevd.names[shock_var]}")
# plt.xlabel("Time Steps")
# plt.ylabel("Response")
# plt.legend()
# plt.grid(True)
# plt.show()

# # Step 5: Compute and plot the Cumulative GIRFs (CIRFs)
# cgirfs = np.cumsum(girfs, axis=0)  # Cumulative sum for each time step

# plt.figure(figsize=(18, 9))
# for i in range(n_vars):
#     plt.plot(cgirfs[:, i], label=f'Cumulative GIRF for variable {fevd.names[i]} in response to shock in {fevd.names[shock_var]}')
# plt.title(f"Cumulative Generalized Impulse Response Functions (CIRFs) for Shock in {fevd.names[shock_var]}")
# plt.xlabel("Time Steps")
# plt.ylabel("Cumulative Response")
# plt.legend()
# plt.grid(True)
# plt.show()

# # Step 4: Plot the GIRFs for the response of each variable to the shock in the 10th variable
# plt.figure(figsize=(12, 6))
# for i in range(n_vars):
#     plt.plot(girfs[:, 0], label=f'GIRF of GBP/USD for variable {fevd.names[i]} in response to shock in {fevd.names[shock_var]}')
# plt.title(f"Generalized Impulse Response Functions (GIRFs) for Shock in {fevd.names[9]}")
# plt.xlabel("Time Steps")
# plt.ylabel("Response")
# plt.grid(True)
# plt.show()

from google.colab import files
fevdnames_all= ['GBP/USD', 'Inflation US', 'Inflation UK', 'Interest Rate US', 'Interest Rate UK', 'Economic Activity US', 'Economic Activity UK', 'M2 US', 'M2 UK','Stock Market News', 'Economic Development News', 'FED News', 'Micro Finance News', 'International Trade News', 'EPU US', 'EPU UK']

import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
from scipy.linalg import sqrtm
from tqdm import tqdm  # Progress bar for bootstrapping
colors_news = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]

# Assuming `results` is your fitted VAR model and `fevd.names` contains variable names
n_steps = 48  # Number of steps for the GIRF
n_vars = results.neqs  # Number of variables in the VAR model
n_bootstraps = 10000  # Number of bootstrap samples for confidence interval estimation

# Step 1: Extract residual covariance matrix (Î£_Îµ) and calculate square root
cov_matrix = results.sigma_u.values if hasattr(results.sigma_u, 'values') else results.sigma_u  # Convert to NumPy array if needed #extracts the residual covariance matrix from the VAR results
sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ

# Step 2: Get the MA coefficient matrices A_n for each time step
ma_coeffs = results.ma_rep(n_steps) #This gives the coefficient matrices An for each time step

# Initialize array to store GIRFs and bootstrap samples
girfs = np.zeros((n_steps, n_vars))
boot_girfs = np.zeros((n_bootstraps, n_steps, n_vars))

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 9
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii) #This ensures that the GIRF is normalized for the shock variable's variance.

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 5, axis=0)
upper_ci = np.percentile(boot_girfs, 95, axis=0)


# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

plt.plot(girfs[:, 0], label=f'GIRF of  GBP/USD in response to 1% shock in {fevd.names[shock_var]}', color=colors_news[0]) #{fevd.names[0]}
plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color=colors_news[0],
        alpha=0.4,
        label="95% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2))
plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevd.names[shock_var]} to {fevdnames_all[0]}")
plt.xlabel("Time Periods After Shock") #Time Steps
plt.ylabel("Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)
plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf")

# # Calculate the 90% confidence intervals
# lower_ci = np.percentile(boot_girfs, 5, axis=0)
# upper_ci = np.percentile(boot_girfs, 95, axis=0)


# # Step 4: Plot the GIRFs with 95% confidence intervals
# # Create a plot
# plt.figure(figsize=(11.69,8.27), dpi=600)

# plt.plot(girfs[:, 0], label=f'GIRF of {fevd.names[0]} in response to 1% shock in {fevd.names[shock_var]}', color=colors_news[0])
# plt.fill_between(
#         range(n_steps),
#         lower_ci[:, 0],
#         upper_ci[:, 0],
#         color=colors_news[0],
#         alpha=0.4,
#         label="90% Confidence Interval")
# plt.axhline(0, color='black', linestyle='--')  # Reference line at 0

# plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevd.names[shock_var]} to {fevd.names[0]}")
# plt.xlabel("Time Periods After Shock") #Time Steps
# plt.ylabel("Response of {fevd.names[0]} (in %)")
# plt.legend()
# plt.grid(True)
# plt.savefig("Generalized Impulse Response function of GBPUSD to Stock Market News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
# plt.show()
# files.download("Generalized Impulse Response function of GBPUSD to Stock Market News.pdf")

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 10
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 4: Plot the GIRFs with 95% confidence intervals
# # Create a plot
# plt.figure(figsize=(11.69,8.27), dpi=600)

# plt.plot(girfs[:, 0], label=f'GIRF of {fevd.names[0]} in response to 1% shock in {fevd.names[shock_var]}', color=colors_news[1])
# plt.fill_between(
#         range(n_steps),
#         lower_ci[:, 0],
#         upper_ci[:, 0],
#         color=colors_news[1],
#         alpha=0.4,
#         label="95% Confidence Interval")
# plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0

# plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevd.names[shock_var]} to {fevd.names[0]}")
# plt.xlabel("Time Periods After Shock") #Time Steps
# plt.ylabel("Response of GBP/USD (in %)")
# plt.legend()
# plt.grid(True)
# plt.savefig("Generalized Impulse Response function of GBPUSD to Economic Development News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
# plt.show()
# files.download("Generalized Impulse Response function of GBPUSD to Economic Development News.pdf")


# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

plt.plot(girfs[:, 0], label=f'GIRF of  GBP/USD in response to 1% shock in {fevd.names[shock_var]}', color=colors_news[1]) #{fevd.names[0]}
plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color=colors_news[1],
        alpha=0.4,
        label="95% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2))
plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevd.names[shock_var]} to {fevdnames_all[0]}")
plt.xlabel("Time Periods After Shock") #Time Steps
plt.ylabel("Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)
plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_Economic_Development_News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_Economic_Development_News.pdf")

# # Calculate the 90% confidence intervals
# lower_ci = np.percentile(boot_girfs, 5, axis=0)
# upper_ci = np.percentile(boot_girfs, 95, axis=0)

# plt.figure(figsize=(11.69,8.27), dpi=600)

# plt.plot(girfs[:, 0], label=f'GIRF of  GBP/USD in response to 1% shock in {fevd.names[shock_var]}', color=colors_news[1]) #{fevd.names[0]}
# plt.fill_between(
#         range(n_steps),
#         lower_ci[:, 0],
#         upper_ci[:, 0],
#         color=colors_news[1],
#         alpha=0.4,
#         label="90% Confidence Interval")
# plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
# plt.xticks(np.arange(0, 49 , step=2))
# plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevd.names[shock_var]} to {fevdnames_all[0]}")
# plt.xlabel("Time Periods After Shock") #Time Steps
# plt.ylabel("Response of GBP/USD (in %)")
# plt.legend()
# plt.grid(True)
# plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_Economic_Development_News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading. #, pad_inches=0.001
# plt.show()
# files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_Economic_Development_News.pdf")

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var =11
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 4: Plot the GIRFs with 95% confidence intervals
# plt.figure(figsize=(12, 6))
# for i in range(n_vars):
#     plt.plot(girfs[:, 0], label=f'GIRF of {fevd.names[i]} in response to shock in {fevd.names[shock_var]}', color="red")
#     plt.fill_between(
#         range(n_steps),
#         lower_ci[:, 0],
#         upper_ci[:, 0],
#         color="blue",
#         alpha=0.2,
#         label="95% CI" if i == 0 else "",
#     )
# plt.title(f"Generalized Impulse Response Functions (GIRFs) with 95% CI for Shock in {fevd.names[shock_var]}")
# plt.xlabel("Time Steps")
# plt.ylabel("Response")
# #plt.legend()
# plt.grid(True)
# plt.show()
plt.figure(figsize=(11.69,8.27), dpi=600)

plt.plot(girfs[:, 0], label=f'GIRF of  GBP/USD in response to 1% shock in {fevd.names[shock_var]}', color=colors_news[2]) #{fevd.names[0]}
plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color=colors_news[2],
        alpha=0.4,
        label="95% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2))
plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevd.names[shock_var]} to {fevdnames_all[0]}")
plt.xlabel("Time Periods After Shock") #Time Steps
plt.ylabel("Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)
plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_FED_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_FED_News.pdf")

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 12
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# # Step 4: Plot the GIRFs with 95% confidence intervals
# plt.figure(figsize=(12, 6))
# for i in range(n_vars):
#     plt.plot(girfs[:, 0], label=f'GIRF of {fevd.names[i]} in response to shock in {fevd.names[shock_var]}', color="red")
#     plt.fill_between(
#         range(n_steps),
#         lower_ci[:, 0],
#         upper_ci[:, 0],
#         color="blue",
#         alpha=0.2,
#         label="95% CI" if i == 0 else "",
#     )
# plt.title(f"Generalized Impulse Response Functions (GIRFs) with 95% CI for Shock in {fevd.names[shock_var]}")
# plt.xlabel("Time Steps")
# plt.ylabel("Response")
# #plt.legend()
# plt.grid(True)
# plt.show()

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

plt.plot(girfs[:, 0], label=f'GIRF of  GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[3]) #{fevd.names[0]}
plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color=colors_news[3],
        alpha=0.4,
        label="95% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2))
plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}")
plt.xlabel("Time Periods After Shock") #Time Steps
plt.ylabel("Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)
plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_Microeconomic_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_Microeconomic_News.pdf")

plt.figure(figsize=(11.69,8.27), dpi=600)

plt.plot(girfs[:, 0], label=f'GIRF of  GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[3]) #{fevd.names[0]}
plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color=colors_news[3],
        alpha=0.4,
        label="95% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2))
plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}")
plt.xlabel("Time Periods After Shock") #Time Steps
plt.ylabel("Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)
plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_Microeconomic_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_Microeconomic_News.pdf")

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 13
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 4: Plot the GIRFs with 95% confidence intervals

plt.figure(figsize=(11.69,8.27), dpi=600)

plt.plot(girfs[:, 0], label=f'GIRF of GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[4]) #{fevd.names[0]}
plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color=colors_news[4],
        alpha=0.4,
        label="95% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2))
plt.title(f"Generalized Impulse Response Functions (GIRFs) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}")
plt.xlabel("Time Periods After Shock") #Time Steps
plt.ylabel("Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)
plt.savefig("Generalized_Impulse_Response_function_of_GBPUSD_to_International_Trade_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Generalized_Impulse_Response_function_of_GBPUSD_to_International_Trade_News.pdf")

import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
from scipy.linalg import sqrtm
from tqdm import tqdm  # For progress bar

# Assuming `results` is your fitted VAR model and `fevd.names` contains variable names
n_steps = 10  # Number of steps for the GIRF
n_vars = results.neqs  # Number of variables in the VAR model
n_bootstraps = 1000  # Number of bootstrap samples for CI estimation

# Step 1: Extract residual covariance matrix (Î£_Îµ) and calculate square root
cov_matrix = results.sigma_u
sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ

# Step 2: Get the MA coefficient matrices A_n for each time step
ma_coeffs = results.ma_rep(n_steps)

# Initialize array to store GIRFs and bootstrap samples
girfs = np.zeros((n_steps, n_vars))
boot_girfs = np.zeros((n_bootstraps, n_steps, n_vars))

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 9  # Index for the 10th variable

# Step 3: Calculate GIRF for the 10th variable
sigma_ii = cov_matrix[shock_var, shock_var]  # The diagonal element Ïƒ_ii for the shock_var
scaling_factor = 1 / np.sqrt(sigma_ii)  # Calculate Ïƒ_ii^(-1/2)

# For each time step, compute the GIRF using the formula
for step in range(n_steps):
    # A_n @ Î£_Îµ @ e_i, where e_i is a selection vector for the shock variable
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response  # Store the response for each variable at this time step

# Step 4: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)

# Step 5: Plot the GIRFs with 95% confidence intervals
plt.figure(figsize=(12, 6))
for i in range(n_vars):
    plt.plot(girfs[:, 0], label=f'GIRF for variable {fevd.names[i]} in response to shock in {fevd.names[shock_var]}', color="blue")
    plt.fill_between(
        range(n_steps),
        lower_ci[:, 0],
        upper_ci[:, 0],
        color="blue",
        alpha=0.2,
        label="95% CI" if i == 0 else "",
    )
plt.title(f"Generalized Impulse Response Functions (GIRFs) with 95% CI for Shock in {fevd.names[shock_var]}")
plt.xlabel("Time Steps")
plt.ylabel("Response")
plt.legend()
plt.grid(True)
plt.show()

# Step 6: Compute and plot the Cumulative GIRFs (CIRFs)
cgirfs = np.cumsum(girfs, axis=0)  # Cumulative sum for each time step

plt.figure(figsize=(12, 6))
for i in range(n_vars):
    plt.plot(cgirfs[:, 0], label=f'Cumulative GIRF for variable {fevd.names[0]} in response to shock in {fevd.names[shock_var]}')
plt.title(f"Cumulative Generalized Impulse Response Functions (CIRFs) for Shock in {fevd.names[shock_var]}")
plt.xlabel("Time Steps")
plt.ylabel("Cumulative Response")
plt.legend()
plt.grid(True)
plt.show()

"""# Cumulative GIRF"""

fevdnames_all= ['GBP/USD', 'Inflation US', 'Inflation UK', 'Interest Rate US', 'Interest Rate UK', 'Economic Activity US', 'Economic Activity UK', 'M2 US', 'M2 UK','Stock Market News', 'Economic Development News', 'FED News', 'Microeconomic News', 'International Trade News', 'EPU US', 'EPU UK']
colors_news = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]
from google.colab import files

girf_values = pd.DataFrame(columns=['Name', 'Value'])

import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from scipy.linalg import sqrtm
from matplotlib.ticker import FormatStrFormatter

# Assuming `results` is your fitted VAR model and `fevd.names` contains variable names
n_steps = 48  # Number of steps for the GIRF
n_vars = results.neqs  # Number of variables in the VAR model
n_bootstraps = 10000  # Number of bootstrap samples for confidence interval estimation

# Step 1: Extract residual covariance matrix (Î£_Îµ) and calculate square root
cov_matrix = results.sigma_u.values if hasattr(results.sigma_u, 'values') else results.sigma_u  # Convert to NumPy array if needed #extracts the residual covariance matrix from the VAR results
sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ

# Step 2: Get the MA coefficient matrices A_n for each time step
ma_coeffs = results.ma_rep(n_steps) #This gives the coefficient matrices An for each time step

# Initialize array to store GIRFs and bootstrap samples
girfs = np.zeros((n_steps, n_vars))

boot_girfs = np.zeros((n_bootstraps, n_steps, n_vars))

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 9  # Index for the 10th variable

# Step 3: Calculate GIRF for the 10th variable
sigma_ii = cov_matrix[shock_var, shock_var]  # The diagonal element Ïƒ_ii for the shock_var
scaling_factor = 1 / np.sqrt(sigma_ii)  # Calculate Ïƒ_ii^(-1/2)

# For each time step, compute the GIRF using the formula
for step in range(n_steps):
    # A_n @ Î£_Îµ @ e_i, where e_i is a selection vector for the shock variable
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response  # Store the response for each variable at this time step

# Step 4: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals

# Ensure that the cumulative GIRFs start at zero for the initial period
#cumulative_girfs[0, :] = 0
#cumulative_boot_girfs[:, 0, :] = 0

# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)


# Step 4: Plot both the original and cumulative GIRFs
plt.figure(figsize=(11.69, 8.27), dpi=600)

# Plot original GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[0])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors_news[0],
    alpha=0.4,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

# Additional plot settings
plt.axhline(0, color='black', linestyle='--')
plt.xticks(np.arange(0, 49, step=2), fontsize=12)
plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))#
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) \n from {fevd.names[shock_var]} to {fevdnames_all[0]}", fontsize=20) #here I added \n  and fontsize 20 instead 16
plt.xlabel("Time Periods After Shock", fontsize=16)
plt.ylabel("Cumulative Response of GBP/USD (in %)", fontsize=16)
plt.legend()
plt.grid(True)

# Save and show plot
plt.savefig("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf", bbox_inches='tight')
plt.show()
files.download("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf")

# Get the current time
current_time = datetime.now()
# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

#new_row = pd.DataFrame({'Name': [fevd.names[shock_var]], 'Value': [cumulative_girfs[i]]})
#girf_values = pd.concat([girf_values, new_row], ignore_index=True)



# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 10
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)

# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)


# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[1])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors_news[1],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))#
plt.yticks(fontsize=12)

plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) \n from {fevd.names[shock_var]} to {fevdnames_all[0]}", fontsize=20)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel("Cumulative Response of GBP/USD (in %)", fontsize=16)
plt.legend()
plt.grid(True)
plt.savefig("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Economic_Development_News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Economic_Development_News.pdf")

# Get the current time
current_time = datetime.now()
# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

#new_row = pd.DataFrame({'Name': [fevd.names[shock_var]], 'Value': [cumulative_girfs[i]]})
#girf_values = pd.concat([girf_values, new_row], ignore_index=True)

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var =11
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)

# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)


# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[2])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors_news[2],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))#
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) \n from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=20)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel("Cumulative Response of GBP/USD (in %)", fontsize=16)
plt.legend()
plt.grid(True)
plt.savefig("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_FED_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_FED_News.pdf")

# Get the current time
current_time = datetime.now()
# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

#new_row = pd.DataFrame({'Name': [fevdnames_all[shock_var]], 'Value': [cumulative_girfs[i]]})
#girf_values = pd.concat([girf_values, new_row], ignore_index=True)

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 12
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)

# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[3])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors_news[3],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))#
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) \n from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=20)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel("Cumulative Response of GBP/USD (in %)", fontsize=16)
plt.legend()
plt.grid(True)

plt.savefig("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Microeconomic_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Microeconomic_News.pdf")

# Get the current time
current_time = datetime.now()
# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

#new_row = pd.DataFrame({'Name': [fevdnames_all[shock_var]], 'Value': [cumulative_girfs[i]]})
#girf_values = pd.concat([girf_values, new_row], ignore_index=True)

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 13
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)

# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of GBP/USD in response to 1% shock in {fevdnames_all[shock_var]}', color=colors_news[4])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors_news[4],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))#
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) \n from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=20)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel("Cumulative Response of GBP/USD (in %)", fontsize=16)
plt.legend()
plt.grid(True)

plt.savefig("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_International_Trade_News.pdf", bbox_inches='tight') #in order not to get the borders when downloading. #, pad_inches=0.001
plt.show()
files.download("Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_International_Trade_News.pdf")

# Get the current time
current_time = datetime.now()
# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

#new_row = pd.DataFrame({'Name': [fevdnames_all[shock_var]], 'Value': [cumulative_girfs[i]]})
#girf_values = pd.concat([girf_values, new_row], ignore_index=True)

# # Initialize the girfs array and boot_girfs array with the correct dimensions
# # n_variables is assumed to be the number of variables in the covariance matrix
# n_variables = cov_matrix.shape[0]  # This represents the number of variables in the model
# girfs = np.zeros((n_steps, n_variables))
# boot_girfs = np.zeros((n_bootstraps, n_steps, n_variables))

# # Loop over shock variables ranging from 1 to 8
# for shock_var in range(1, 9):  # Adjusted for shock variables from 1 to 8

#     # Define specific shock variable
#     sigma_ii = cov_matrix[shock_var, shock_var]
#     scaling_factor = 1 / np.sqrt(sigma_ii)

#     # Step 1: Calculate the GIRF for the selected shock variable
#     for step in range(n_steps):
#         response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
#         girfs[step, :] = response

#     # Step 2: Bootstrap to calculate confidence intervals
#     for b in tqdm(range(n_bootstraps), desc=f"Bootstrapping for shock variable {shock_var}"):
#         # Resample residuals and fit the VAR model to generate new GIRFs
#         resampled_data = results.simulate_var(steps=len(results.resid))
#         bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
#         bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

#         # Compute GIRF for this bootstrap sample
#         for step in range(n_steps):
#             response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
#             boot_girfs[b, step, :] = response

#     # Calculate the 95% confidence intervals
#     lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
#     upper_ci = np.percentile(boot_girfs, 97.5, axis=0)

#     # Step 3: Calculate cumulative GIRFs
#     cumulative_girfs = np.cumsum(girfs, axis=0)
#     cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)

#     # Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
#     lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
#     upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

#     # Step 4: Plot the GIRFs with 95% confidence intervals
#     plt.figure(figsize=(11.69,8.27), dpi=600)

#     # Plot cumulative GIRF
#     plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of GBP/USD to {fevdnames_all[shock_var]}', color=colors_news[shock_var % len(colors_news)])
#     plt.fill_between(
#         range(n_steps),
#         lower_ci_cumulative[:, 0],
#         upper_ci_cumulative[:, 0],
#         color=colors_news[shock_var % len(colors_news)],
#         alpha=0.3,
#         label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
#     )

#     plt.axhline(0, color='black', linestyle='--')
#     plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
#     plt.yticks(fontsize=12)
#     plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
#     plt.xlabel("Time Periods After Shock", fontsize=16)
#     plt.ylabel("Cumulative Response of GBP/USD (in %)", fontsize=16)
#     plt.legend()
#     plt.grid(True)

#     # Save the plot
#     plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
#     plt.savefig(plot_filename, bbox_inches='tight')
#     plt.show()

#     # Optionally, download the file (if in a suitable environment like Google Colab)
#     try:
#         files.download(plot_filename)
#     except:
#         print(f"Saved {plot_filename}")

"""## Cumulative GIRFS for macro"""

current_time = datetime.now()

import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from scipy.linalg import sqrtm
# Assuming `results` is your fitted VAR model and `fevd.names` contains variable names
n_steps = 48  # Number of steps for the GIRF
n_vars = results.neqs  # Number of variables in the VAR model
n_bootstraps = 10000  # Number of bootstrap samples for confidence interval estimation

# Step 1: Extract residual covariance matrix (Î£_Îµ) and calculate square root
cov_matrix = results.sigma_u.values if hasattr(results.sigma_u, 'values') else results.sigma_u  # Convert to NumPy array if needed #extracts the residual covariance matrix from the VAR results
sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ

# Step 2: Get the MA coefficient matrices A_n for each time step
ma_coeffs = results.ma_rep(n_steps) #This gives the coefficient matrices An for each time step

# Initialize array to store GIRFs and bootstrap samples
girfs = np.zeros((n_steps, n_vars))

boot_girfs = np.zeros((n_bootstraps, n_steps, n_vars))

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 1
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 2
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 3
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 4
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 5
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 6
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 7
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

!ls

# Define the specific shock variable (10th variable, so index 9 in Python)
shock_var = 8
sigma_ii = cov_matrix[shock_var, shock_var]
scaling_factor = 1 / np.sqrt(sigma_ii)
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
# Calculate the GIRF for the 10th variable
for step in range(n_steps):
    response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
    girfs[step, :] = response

# Step 3: Bootstrap to calculate confidence intervals
for b in tqdm(range(n_bootstraps), desc="Bootstrapping"):
    # Resample residuals and fit the VAR model to generate new GIRFs
    resampled_data = results.simulate_var(steps=len(results.resid))
    bootstrap_model = VAR(resampled_data).fit(maxlags=results.k_ar)
    bootstrap_ma_coeffs = bootstrap_model.ma_rep(n_steps)

    # Compute GIRF for this bootstrap sample
    for step in range(n_steps):
        response = bootstrap_ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        boot_girfs[b, step, :] = response

# Calculate the 95% confidence intervals
lower_ci = np.percentile(boot_girfs, 2.5, axis=0)
upper_ci = np.percentile(boot_girfs, 97.5, axis=0)


# Step 3: Calculate cumulative GIRFs
cumulative_girfs = np.cumsum(girfs, axis=0)  # Calculate cumulative GIRFs
cumulative_boot_girfs = np.cumsum(boot_girfs, axis=1)  # Bootstrap cumulative GIRFs for confidence intervals


# Calculate cumulative 95% confidence intervals from bootstrapped GIRFs
lower_ci_cumulative = np.percentile(cumulative_boot_girfs, 5, axis=0)
upper_ci_cumulative = np.percentile(cumulative_boot_girfs, 95, axis=0)

# Step 4: Plot the GIRFs with 95% confidence intervals
# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot cumulative GIRF
plt.plot(cumulative_girfs[:, 0], label=f'Cumulative GIRF of {fevdnames_all[0]} in response to 1% shock in {fevdnames_all[shock_var]}', color=colors[shock_var])
plt.fill_between(
    range(n_steps),
    lower_ci_cumulative[:, 0],
    upper_ci_cumulative[:, 0],
    color=colors[shock_var],
    alpha=0.3,
    label="95% Bootstrap Confidence Interval for Cumulative GIRF (10,000 Simulations)"
)

plt.axhline(0, color='black', linestyle='--')  # Reference line at 0
plt.xticks(np.arange(0, 49 , step=2), fontsize=12)
plt.yticks(fontsize=12)
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {fevdnames_all[shock_var]} to {fevdnames_all[0]}", fontsize=16)
plt.xlabel("Time Periods After Shock", fontsize=16) #Time Steps
plt.ylabel(f"Cumulative Response of {fevdnames_all[0]} (in %)", fontsize=16)
plt.legend()
plt.grid(True)
# Save the plot
plot_filename = f"Cumulative_GIRF_of_GBPUSD_to_{fevdnames_all[shock_var]}.pdf"
plt.savefig(plot_filename, bbox_inches='tight')
plt.show()
files.download(plot_filename)

# Print the current time
print("Current Time:", current_time.strftime("%Y-%m-%d %H:%M")) #:%S

"""# FEVDS from GIRFs as done for EUR/USD"""

import numpy as np
from scipy.linalg import sqrtm

# Parameters
n_steps = 48  # Number of time steps
n_vars = 16  # Number of variables (shocks)

# Placeholder for data (replace with actual)
cov_matrix = np.random.rand(n_vars, n_vars)  # Replace with actual covariance matrix
ma_coeffs = np.random.rand(n_steps, n_vars, n_vars)  # Replace with actual MA coefficients
girfs = np.zeros((n_vars, n_vars, n_steps))  # Store GIRFs: shape (shocks, responses, time steps)
cumulative_girfs = np.zeros_like(girfs)  # Store cumulative GIRFs

# Compute GIRFs for all shock variables
for shock_var in range(n_vars):  # Loop over all shock variables
    sigma_ii = cov_matrix[shock_var, shock_var]
    scaling_factor = 1 / np.sqrt(sigma_ii)  # Normalize for shock variable's variance

    for step in range(n_steps):
        # Calculate the response for each step
        response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
        girfs[shock_var, :, step] = response  # Store the GIRF

# Calculate cumulative GIRFs for all shocks and responses
cumulative_girfs = np.cumsum(girfs, axis=2)

# Output shapes for verification
print("GIRFs shape:", girfs.shape)  # Expected: (n_vars, n_vars, n_steps)
print("Cumulative GIRFs shape:", cumulative_girfs.shape)  # Expected: (n_vars, n_vars, n_steps)

# Example: Print GIRFs and cumulative GIRFs for the first shock variable
shock_var_index = 0
print("GIRFs for Shock Variable 1:")
print(girfs[shock_var_index])
print("Cumulative GIRFs for Shock Variable 1:")
print(cumulative_girfs[shock_var_index])

# Output shapes for verification
print("GIRFs shape:", girfs.shape)  # Expected: (n_vars, n_vars, n_steps)
print("Cumulative GIRFs shape:", cumulative_girfs.shape)  # Expected: (n_

import numpy as np

# Assume cumulative_girfs is of shape (n_vars, n_vars, n_steps)
# cumulative_girfs[shock_var, response_var, time_step]

def calculate_fevd(cumulative_girfs, response_var_index):
    """
    Calculate FEVD for a specific response variable.

    Args:
    cumulative_girfs (np.ndarray): Cumulative GIRFs of shape (n_shocks, n_responses, n_steps)
    response_var_index (int): Index of the response variable (e.g., 0 for the first variable)

    Returns:
    np.ndarray: FEVD of shape (n_shocks, n_steps)
    """
    n_shocks, n_responses, n_steps = cumulative_girfs.shape
    fevd = np.zeros((n_shocks, n_steps))

    # Calculate FEVD for the specified response variable
    for t in range(n_steps):
        # Numerator: Contribution of each shock
        shock_contributions = np.sum(cumulative_girfs[:, response_var_index, :t+1] ** 2, axis=1)

        # Denominator: Total contribution from all shocks
        total_variance = np.sum(shock_contributions)

        # FEVD for each shock at time t
        fevd[:, t] = shock_contributions / total_variance

    return fevd

# Example usage
response_var_index = 0  # First variable
fevd_first_variable = calculate_fevd(cumulative_girfs, response_var_index)

# Output shape
print("FEVD Shape:", fevd_first_variable.shape)  # Expected: (n_shocks, n_steps)

# Example: Print FEVD for the first shock
print("FEVD for the first variable (all shocks):")
print(fevd_first_variable)

# Plot FEVD for the first variable
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
for shock_var in range(fevd_first_variable.shape[0]):
    plt.plot(fevd_first_variable[shock_var, :], label=f'{data.columns[shock_var]}')  #Shock {shock_var+1}
plt.title('FEVD for the First Variable')
plt.xlabel('Time Steps')
plt.ylabel('FEVD Contribution')
plt.legend()
plt.grid(True)
plt.show()

fevd_first_variable[0,:]* 100

from google.colab import files

num_variables = fevd_first_variable.shape[0]

print(f"The sum of the last 7 values is: {fevd_first_variable[0, :]* 100}")

# Calculate the sum of the last 7 variables (All News Variables)
sum_all_news = np.sum(fevd_first_variable[-7:, :], axis=0)

# Print the sum
print(f"The sum of the last 7 values is: {sum_all_news}")

# Calculate the sum of variables 2 through 9 (All Macroeconomic Variables)
sum_macro_variables = np.sum(fevd_first_variable[1:9, :], axis=0)  # Index 1 to 8 corresponds to variables 2 to 9
print(f"The sum of macro: {sum_macro_variables}")

# Colors for plotting
colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
          "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# Plotting
plt.figure(figsize=(11.69,8.27), dpi=600)

# # Plot each time series as an area graph for individual variables
# for i in range(first_elements_all.shape[0]):
#     plt.fill_between(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
#                      label=fevd.names[i], color=colors[i % len(colors)], alpha=0.3)  # Original variables

# Extract the first variable, the FX
#first_variable = fevd_first_variable[0, :]
# plt.fill_between(np.arange(0, first_variable.shape[0] ), first_variable * 100,
#                  label='EUR/USD', color=colors[0], alpha=0.7)

# Plot the sum of All Macroeconomic Variables
plt.fill_between(np.arange(0, sum_macro_variables.shape[0] ), sum_macro_variables * 100,
                 label='All Macroeconomic Variables', color='#FF7F50', alpha=0.7)

# Plot the sum of All News Variables
plt.fill_between(np.arange(0, sum_all_news.shape[0]), sum_all_news * 100,
                 label='All News Variables', color='yellow', alpha=0.7)
plt.fill_between(np.arange(0, fevd_first_variable[0, :].shape[0] ), fevd_first_variable[0, :]* 100, label=f'{data.columns[0]}', alpha=0.7)  #Shock {shock_var+1}


plt.title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
plt.legend()
plt.grid(True)
plt.xticks(np.arange(0, fevd_first_variable.shape[1]+1 , step=2), fontsize=12)  # Set x-ticks to start from 1
plt.xlim(0, fevd_first_variable.shape[1])  # Set x-limits to start from 1

# Set y-axis ticks to show every 5 units
plt.yticks(np.arange(0, 101, 10), fontsize=12)
plt.savefig('FEVD_based_on_cum_girf_for_GBP_USD_Area_Plot.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("FEVD_based_on_cum_girf_for_GBP_USD_Area_Plot.pdf")

from google.colab import files

plt.figure(figsize=(11.69,8.27), dpi=600)


# Normalize the values so they add up to 100% at each time point
stacked_data = np.vstack([fevd_first_variable[0, :], sum_all_news, sum_macro_variables])
stacked_data_normalized = stacked_data / stacked_data.sum(axis=0)

# Colors for each group
colors = ["#00338D", "yellow", "#FF7F50"]

# Plotting
plt.figure(figsize=(11.69, 8.27), dpi=600)

# Create the stacked area plot with normalized values
plt.stackplot(np.arange(stacked_data_normalized.shape[1]),
              stacked_data_normalized * 100,  # Scale to 100%
              labels=["EUR/USD", "All News Variables", "All Macroeconomic Variables"],
              colors=colors, alpha=0.7)
plt.title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
plt.xlabel('Time (Months)', fontsize=16)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
# Create a second y-axis (on the right)
plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)


plt.legend()
plt.grid(True)
plt.xticks(np.arange(0, stacked_data.shape[1]+1 , step=2), fontsize=12)  # Set x-ticks to start from 1
plt.xlim(0, stacked_data.shape[1])  # Set x-limits to start from 1

# Set y-axis ticks to show every 5 units
plt.yticks(np.arange(0, 101, 10), fontsize=12)
plt.savefig('FEVD_based_cum_girf_for_GBP_USD_Area_Plot.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("FEVD_based_cum_girf_for_GBP_USD_Area_Plot.pdf")

# import numpy as np
# from scipy.linalg import sqrtm
# # def calculate_fevd(girfs):
# #     # Get the number of variables (n_vars), shocks (n_shocks), and time periods (n_steps)
# #     n_steps, n_vars = girfs.shape  # Adjust dimensions if needed (assuming girfs are [n_steps, n_vars])
# #     fevd = np.zeros((n_vars, n_vars, n_steps))  # FEVD matrix, for each shock variable and time period

# #     # Loop through each variable (target variable) and each shock
# #     for i in range(n_vars):  # Loop over all variables (target variables)
# #         for j in range(n_vars):  # Loop over all shocks
# #             # Calculate the variance of the GIRF for the target variable i, from shock j, across all periods
# #             var_i = np.var(girfs[:, i])  # Variance of impulse response for target variable i

# #             # Compute the total variance (sum of variances across all variables from shock j)
# #             total_variance = np.sum([np.var(girfs[:, k]) for k in range(n_vars)])  # Sum over all variables

# #             # Calculate the FEVD for each period
# #             fevd[i, j, :] = var_i / total_variance  # Contribution of shock j to the variance of variable i

# #     return fevd


# n_steps = 48
# n_vars =  results.neqs  # data.shape[1]  # Assuming 'data' is your dataframe

# # Initialize girfs with dummy data or actual calculated GIRFs
# #girfs = np.random.rand(n_steps, n_vars)  # Replace with actual GIRFs

# # Initialize arrays to store GIRFs and bootstrap samples
# #boot_girfs = np.zeros((n_bootstraps, n_steps, n_vars))  # Bootstrap GIRFs

# # Define shock variable indices and covariance matrix (adjust these according to your data)
# cov_matrix = np.random.rand(n_vars, n_vars)  # Example covariance matrix, replace with actual
# sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ
# ma_coeffs = np.random.rand(n_steps, n_vars)  # Example MA coefficients, replace with actual
# girfs = np.zeros((n_steps, n_vars))  # Store GIRFs for each variable

# # Loop through each variable and calculate the GIRF
# for shock_var in range(n_vars):  # Loop over all variables
#     sigma_ii = cov_matrix[shock_var, shock_var]
#     scaling_factor = 1 / np.sqrt(sigma_ii)  # Normalize for shock variable's variance

#     for step in range(n_steps):
#         response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
#         girfs[step, shock_var] = response  # Store the response for the current shock variable

#     # # Bootstrap sampling (if required for bootstrapping GIRFs)
#     # for b in range(n_bootstraps):
#     #     # Assuming bootstrap involves resampling your residuals or shock data
#     #     # Here we assume a simple resampling method, modify according to your needs
#     #     bootstrap_residuals = np.random.choice(cov_matrix.flatten(), size=cov_matrix.size, replace=True)
#     #     bootstrap_cov_matrix = bootstrap_residuals.reshape(cov_matrix.shape)

#     #     # Recalculate GIRFs for bootstrap sample
#     #     for step in range(n_steps):
#     #         response = ma_coeffs[step] @ bootstrap_cov_matrix[:, shock_var] * scaling_factor
#     #         boot_girfs[b, step, shock_var] = response  # Store bootstrap response

# # Calculate FEVD from GIRFs

# # fevd = calculate_fevd(girfs)
# # print(fevd)

# import numpy as np
# ###11.12.
# from scipy.linalg import sqrtm

# n_steps = 48  # Number of time steps (steps ahead)
# n_vars = results.neqs  # Number of variables (shocks) in the VAR model

# # Initialize girfs with zeros to store the GIRFs for each variable
# girfs = np.zeros((n_steps, n_vars))  # Store GIRFs for each variable, shape (n_steps, n_vars)

# # Define shock variable indices and covariance matrix (adjust these according to your data)
# cov_matrix = np.random.rand(n_vars, n_vars)  # Example covariance matrix, replace with actual
# sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ (residual covariance matrix)
# ma_coeffs = np.random.rand(n_steps, n_vars)  # Example MA coefficients, replace with actual

# # Loop through each variable and calculate the GIRF
# for shock_var in range(n_vars):  # Loop over all variables (shocks)
#     sigma_ii = cov_matrix[shock_var, shock_var]
#     scaling_factor = 1 / np.sqrt(sigma_ii)  # Normalize for shock variable's variance

#     for step in range(n_steps):
#         response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
#         girfs[step, shock_var] = response  # Store the response for the current shock variable

# # After the loop, girfs will contain the GIRFs for all variables (all shocks) over all time steps.
# # You can inspect or plot the results now

# # Example: Plot the GIRFs for each variable
# import matplotlib.pyplot as plt

# plt.figure(figsize=(12, 6))
# for shock_var in range(n_vars):
#     plt.plot(np.arange(n_steps), girfs[:, shock_var], label=f'Variable {shock_var+1}')

# plt.title('GIRFs for All Variables')
# plt.xlabel('Time Steps')
# plt.ylabel('Impulse Response')
# plt.legend()
# plt.grid(True)
# plt.show()

girfs

girfs[:, 0]

import numpy as np
from scipy.linalg import sqrtm
import matplotlib.pyplot as plt

# Assuming you have the fitted VAR model results
n_steps = 48  # Number of time steps (steps ahead)
n_vars = results.neqs  # Number of variables (shocks) in the VAR model

# Initialize girfs with zeros to store the GIRFs for each shock and response variable
girfs = np.zeros((n_vars, n_vars, n_steps))  # Shape (n_vars, n_vars, n_steps)

# Define shock variable indices and covariance matrix (adjust these according to your data)
cov_matrix = np.random.rand(n_vars, n_vars)  # Example covariance matrix, replace with actual
sqrt_cov_matrix = sqrtm(cov_matrix)  # Square root of Î£_Îµ (residual covariance matrix)
ma_coeffs = np.random.rand(n_steps, n_vars)  # Example MA coefficients, replace with actual

# Loop through each shock variable and calculate the GIRF for each response variable
for shock_var in range(n_vars):  # Loop over all shock variables
    sigma_ii = cov_matrix[shock_var, shock_var]
    scaling_factor = 1 / np.sqrt(sigma_ii)  # Normalize for shock variable's variance

    for response_var in range(n_vars):  # Loop over all response variables
        for step in range(n_steps):
            response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
            girfs[response_var, shock_var, step] = response  # Store response for shock_var affecting response_var
            cum_girfs= np.cumsum(girfs, axis=0)

# Now girfs contains the impulse responses for all shock-response pairs over time
# You can plot the GIRFs for each response variable

# plt.figure(figsize=(12, 6))

# # Plot the GIRFs for each shock variable's effect on all response variables over time
# for shock_var in range(n_vars):
#     for response_var in range(n_vars):
#         plt.plot(np.arange(n_steps), girfs[response_var, shock_var, :], label=f'Shock {shock_var+1} -> Response {response_var+1}')

# plt.title('GIRFs for All Shock-Response Variables')
# plt.xlabel('Time Steps')
# plt.ylabel('Impulse Response')
# plt.legend()
# plt.grid(True)
# plt.show()

all_colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44", "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333", "#EBBB17", "#F63D2E"]

plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the GIRFs for each shock variable's effect on all response variables over time
i=0
for shock_var in range(n_vars):
   plt.plot(np.arange(n_steps), girfs[0, shock_var, :], label=f'Shock {shock_var+1} -> Response {0+1}', color=all_colors[i])
   i=i+1

plt.title('GIRFs for GBP/USD')
plt.xlabel('Time Steps')
plt.ylabel('Impulse Response')
plt.legend()
plt.grid(True)
plt.savefig('All_GIRFs GBPUSD.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("All_GIRFs GBPUSD.pdf")

all_colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44", "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333", "#EBBB17", "#F63D2E"]

plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the GIRFs for each shock variable's effect on all response variables over time
i=0
for shock_var in range(n_vars):
   plt.plot(np.arange(n_steps), girfs[0, shock_var, :], label=f'Shock {shock_var+1} -> Response {0+1}', color=all_colors[i])
   i=i+1

plt.title('GIRFs for GBP/USD')
plt.xlabel('Time Steps')
plt.ylabel('Impulse Response')
plt.legend()
plt.grid(True)
plt.savefig('All_GIRFs GBPUSD.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("All_GIRFs GBPUSD.pdf")

import numpy as np

def calculate_fevd_from_irf(girfs):
    """
    Calculate FEVD from IRFs.

    Parameters:
    - irfs: 3D array of IRFs with shape (n_vars, n_shocks, n_steps)

    Returns:
    - fevd: 3D array of FEVD with shape (n_vars, n_shocks, n_steps)
    """
    n_vars, n_shocks, n_steps = girfs.shape
    fevd = np.zeros((n_vars, n_shocks, n_steps))

    for h in range(n_steps):  # Horizon
        for i in range(n_vars):  # Target variable
            total_variance = 0

            # Calculate total variance for variable i at horizon h
            for j in range(n_shocks):  # Shocks
                total_variance += np.sum(girfs[i, j, :h+1]**2)  # Sum of squared IRFs

            for j in range(n_shocks):  # Shocks
                # Contribution of shock j to variance of variable i at horizon h
                contribution = np.sum(girfs[i, j, :h+1]**2)
                fevd[i, j, h] = contribution / total_variance  # FEVD

    return fevd

# Example Usage:
# Assuming 'irfs' is a 3D array with shape (n_vars, n_shocks, n_steps)
n_vars, n_shocks, n_steps = 16, 16, 48  # Example dimensions
#irfs = np.random.rand(n_vars, n_shocks, n_steps)  # Example IRFs
fevd = calculate_fevd_from_irf(girfs) #cum_

# Print the FEVD for the first variable
print("FEVD for the first variable over time:")
print(fevd[0, :, :])

import matplotlib.pyplot as plt

# FEVD for the first variable
all_colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44", "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333", "#EBBB17", "#F63D2E"]

plt.figure(figsize=(11.69,8.27), dpi=600)

fevd_first_var = fevd[0, :, :]  # Shape (n_shocks, n_steps)

# Plot contributions of each shock to the first variable
plt.figure(figsize=(11.69,8.27), dpi=600)
i=0
for shock in range(fevd_first_var.shape[0]):
    plt.plot(fevd_first_var[shock, :], label=f'Shock {shock+1}', color=all_colors[i])
    i=i+1
plt.title('FEVD for First Variable')
plt.xlabel('Forecast Horizon')
plt.ylabel('Variance Contribution')
plt.legend()
plt.grid(True)
plt.savefig('FEVDs_based_on_GIRFs_GBPUSD.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("FEVDs_based_on_GIRFs_GBPUSD.pdf")
plt.show()

# wrong
# import numpy as np
# import matplotlib.pyplot as plt

# # Assuming 'girfs' is the 2D array of GIRFs with shape (n_steps, n_vars)
# # n_steps = number of time steps, n_vars = number of variables (shocks)

# def calculate_fevd(girfs):
#     n_steps, n_vars = girfs.shape  # Time steps and number of variables
#     fevd = np.zeros((n_vars, n_steps))  # FEVD matrix: variables x time steps

#     for t in range(n_steps):
#         # Calculate the variance of the GIRF for each variable at time t
#         var_i = np.var(girfs[t, :])  # Variance of the GIRF at time t (all variables)

#         # Compute the total variance (sum of variances across all variables at time t)
#         total_variance = np.sum(np.var(girfs, axis=0))  # Sum over all variables

#         # Calculate the FEVD for each time step
#         fevd[:, t] = var_i / total_variance  # Contribution of each shock

#     return fevd

# # Example: Assuming 'girfs' is already calculated and has the shape (n_steps, n_vars)
# n_steps = 48  # Number of steps (horizon)
# n_vars = girfs.shape[1]  # Number of variables (shocks)

# # Calculate FEVD for each variable at each time step
# fevd_values = calculate_fevd(girfs)

# # Print the FEVD for the first variable at each time step
# print("FEVD for the first variable over time:")
# print(fevd_values[0, :])

# # Plot the FEVD for the first variable
# plt.plot(fevd_values[:, :])  # FEVD over time for the first variable
# plt.xlabel('Time Steps')
# plt.ylabel('FEVD')
# plt.title('FEVD of the First Variable')
# plt.show()

# import numpy as np
# from scipy.linalg import sqrtm

# def calculate_girf(ma_coeffs, cov_matrix, n_steps, n_vars):
#     # Initialize array to store GIRFs for each variable (shock and target)
#     girfs = np.zeros((n_steps, n_vars))

#     # Loop through each shock variable (j) and calculate GIRF
#     for shock_var in range(n_vars):  # Loop over all variables (shocks)
#         sigma_ii = cov_matrix[shock_var, shock_var]  # Variance of the shock
#         scaling_factor = 1 / np.sqrt(sigma_ii)  # Normalize for shock variable's variance

#         # Loop through each time step (t) and calculate the response to the shock
#         for step in range(n_steps):
#             # GIRF calculation for each time step
#             response = ma_coeffs[step] @ cov_matrix[:, shock_var] * scaling_factor
#             girfs[step, shock_var] = response  # Store the response for the shock variable at time step

#     return girfs

# # Assuming girfs is your generalized impulse response functions
# # Define number of time steps (n_steps) and variables (n_vars)
# n_steps = 48  # Set the number of steps (horizons)
# n_vars = data.shape[1]  # Assuming 'data' is your dataframe, adjust for number of variables

# # Example: Using dummy covariance matrix and MA coefficients (replace with actual values)
# cov_matrix = np.random.rand(n_vars, n_vars)  # Example covariance matrix (use actual data)
# ma_coeffs = np.random.rand(n_steps, n_vars)  # Example MA coefficients (use actual model)

# # Calculate the GIRFs for each shock variable and time step
# girfs = calculate_girf(ma_coeffs, cov_matrix, n_steps, n_vars)

# # Now you can proceed to calculate FEVD based on the computed GIRFs

# import numpy as np
# import matplotlib.pyplot as plt
# from google.colab import files

# first_elements_all = [fevd[0, :, i] for i in range(fevd.shape[0])]

# # Convert to a 2D NumPy array for easier manipulation
# first_elements_all = np.array(first_elements_all)

# # Identifying the number of variables and time points
# num_variables = first_elements_all.shape[0]

# # Calculate the sum of the last 7 variables (All News Variables)
# sum_all_news = np.sum(first_elements_all[-7:, :], axis=0)

# # Calculate the sum of variables 2 through 9 (All Macroeconomic Variables)
# sum_macro_variables = np.sum(first_elements_all[1:9, :], axis=0)  # Index 1 to 8 corresponds to variables 2 to 9

# # Colors for plotting
# colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44",
#           "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933",
#           "#BC204B", "#333333", "#EAAA00", "#F68D2E"]

# # Plotting
# plt.figure(figsize=(11.69,8.27), dpi=600)

# # # Plot each time series as an area graph for individual variables
# # for i in range(first_elements_all.shape[0]):
# #     plt.fill_between(np.arange(1, first_elements_all.shape[1] + 1), first_elements_all[i] * 100,
# #                      label=fevd.names[i], color=colors[i % len(colors)], alpha=0.3)  # Original variables

# # Extract the first variable, the FX
# first_variable = first_elements_all[0, :]
# plt.plot(np.arange(first_variable.shape[0]), first_variable * 100, label='GBP/USD', color='#00338D', linewidth=2.5)
# plt.plot(np.arange(sum_all_news.shape[0]), sum_all_news * 100, label='All News Variables', color='yellow', linewidth=2.5)
# plt.plot(np.arange(sum_macro_variables.shape[0]), sum_macro_variables * 100, label='All Macroeconomic Variables', color='#FF7F50', linewidth=2.5)


# # plt.fill_between(np.arange(0, first_variable.shape[0]), first_variable * 100,
# #                  label='x', color=colors[0], alpha=0.7)

# # # Plot the sum of All Macroeconomic Variables
# # plt.fill_between(np.arange(0, sum_macro_variables.shape[0] ), sum_macro_variables * 100,
# #                  label='All Macroeconomic Variables', color='#FF7F50', alpha=0.7)

# # # Plot the sum of All News Variables
# # plt.fill_between(np.arange(0, sum_all_news.shape[0]), sum_all_news * 100,
# #                  label='All News Variables', color='yellow', alpha=0.7)


# plt.title('FEVD for GBP/USD', fontweight='bold', fontsize=16)
# plt.xlabel('Time (Months)', fontsize=16)
# plt.ylabel('Contribution (%) to GBP/USD', fontsize=16)
# plt.legend()
# plt.grid(True)
# plt.xticks(np.arange(0, first_elements_all.shape[1]+1 , step=2), fontsize=12)  # Set x-ticks to start from 1
# plt.xlim(0, first_elements_all.shape[1])  # Set x-limits to start from 1

# # Set y-axis ticks to show every 5 units
# plt.yticks(np.arange(0, 101, 10), fontsize=12)
# plt.savefig('FEVD_for_GBP_USD_Area_Plot_girf_based.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
# plt.show()
# files.download("FEVD_for_GBP_USD_Area_Plot_girf_based.pdf")

girf_first_variable = girfs[:, 0]  # First column represents the GIRF for the first variable
girf_first_variable
girfs.shape

all_colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44", "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333", "#EBBB17", "#F63D2E"]

plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the GIRFs for each shock variable's effect on all response variables over time
i=9
for shock_var in range(n_vars):
   plt.plot(np.arange(n_steps), girfs[0, shock_var, :], label=f'data.columns[shock_var+1]', color=all_colors[i])
   i=i+1

plt.title('GIRFs for GBP/USD')
plt.xlabel('Time Steps')
plt.ylabel('Impulse Response')
plt.legend()
plt.grid(True)
plt.savefig('All_GIRFs GBPUSD.pdf', bbox_inches='tight') #in order not to get the borders when downloading.
plt.show()
files.download("All_GIRFs GBPUSD.pdf")

"""# Local Projections"""

def local_projection_irf(data, shock_var, response_var, shock_size=1, horizons=10):
    """
    Compute local projection IRFs for a VAR model.

    Parameters:
        data (pd.DataFrame): DataFrame containing the variables.
        shock_var (str): Name of the variable to shock.
        response_var (str): Name of the variable to observe the response.
        shock_size (float): Size of the shock applied to the shock variable.
        horizons (int): Number of horizons (periods) to compute IRFs for.

    Returns:
        pd.Series: IRF values for the response variable.
    """
    irfs = []
    data_with_shock = data.copy()

    # Apply shock to the shock variable
    data_with_shock[shock_var] += shock_size

    # Loop over each horizon
    for h in range(1, horizons + 1):
        # Shift the data for horizon h
        response = data[response_var].shift(-h)
        regressors = sm.add_constant(data_with_shock)

        # Run OLS regression
        model = sm.OLS(response, regressors, missing='drop').fit()
        irf_value = model.params[shock_var]
        irfs.append(irf_value)

    return pd.Series(irfs, index=range(1, horizons + 1))

data

shock_var = 'Stock Market News'  # Variable receiving the shock
response_var = 'GBPUSD'  # Variable whose response is measured
horizons = 48  # Number of periods to forecast

# Compute the IRF using local projections
irf_values = local_projection_irf(data, shock_var, response_var, shock_size=1, horizons=horizons)
print(irf_values)

#confidence bands
num_bootstrap = 1000  # Number of bootstrap samples
confidence_level = 0.95  # Desired confidence level (e.g., 95%)

# Function to compute IRF with bootstrapping for confidence intervals
def bootstrap_irf(data, shock_var, response_var, shock_size=1, horizons=10, num_bootstrap=1000):
    irf_matrix = []

    # Compute the original IRF
    original_irf = local_projection_irf(data, shock_var, response_var, shock_size=shock_size, horizons=horizons)

    # Run bootstrapping
    for _ in range(num_bootstrap):
        # Resample data with replacement
        boot_data = data.sample(frac=1, replace=True)

        # Compute IRF on bootstrapped data
        boot_irf = local_projection_irf(boot_data, shock_var, response_var, shock_size=shock_size, horizons=horizons)
        irf_matrix.append(boot_irf)

    # Convert to numpy array for easy manipulation
    irf_matrix = np.array(irf_matrix)

    # Calculate percentiles for confidence intervals
    lower_bound = np.percentile(irf_matrix, (1 - confidence_level) / 2 * 100, axis=0)
    upper_bound = np.percentile(irf_matrix, (1 + confidence_level) / 2 * 100, axis=0)

    return original_irf, lower_bound, upper_bound

# Compute IRF and confidence intervals
irf_values, lower_bound, upper_bound = bootstrap_irf(data, shock_var, response_var, shock_size=1, horizons=horizons, num_bootstrap=num_bootstrap)



# Plot the IRF with confidence bands in your specified format
plt.figure(figsize=(8, 6))
plt.plot(irf_values, marker='o', linestyle='-', label='IRF')
plt.fill_between(range(horizons), lower_bound, upper_bound, color='gray', alpha=0.3, label=f"{int(confidence_level*100)}% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Add horizontal line at y=0
plt.xticks(np.arange(0, horizons, step=2))  # Set x-ticks as specified
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {shock_var} to {response_var}")
plt.xlabel("Time Periods After Shock")
plt.ylabel("Cumulative Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)

# Save and show plot
filename = "Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf"
plt.savefig(filename, bbox_inches='tight')
plt.show()

data



shock_var = 'Economic Development News'  # Variable receiving the shock
response_var = 'GBPUSD'  # Variable whose response is measured
horizons = 48  # Number of periods to forecast

# Compute the IRF using local projections
irf_values = local_projection_irf(data, shock_var, response_var, shock_size=1, horizons=horizons)
print(irf_values)

#confidence bands
num_bootstrap = 1000  # Number of bootstrap samples
confidence_level = 0.95  # Desired confidence level (e.g., 95%)

# Function to compute IRF with bootstrapping for confidence intervals
def bootstrap_irf(data, shock_var, response_var, shock_size=1, horizons=10, num_bootstrap=1000):
    irf_matrix = []

    # Compute the original IRF
    original_irf = local_projection_irf(data, shock_var, response_var, shock_size=shock_size, horizons=horizons)

    # Run bootstrapping
    for _ in range(num_bootstrap):
        # Resample data with replacement
        boot_data = data.sample(frac=1, replace=True)

        # Compute IRF on bootstrapped data
        boot_irf = local_projection_irf(boot_data, shock_var, response_var, shock_size=shock_size, horizons=horizons)
        irf_matrix.append(boot_irf)

    # Convert to numpy array for easy manipulation
    irf_matrix = np.array(irf_matrix)

    # Calculate percentiles for confidence intervals
    lower_bound = np.percentile(irf_matrix, (1 - confidence_level) / 2 * 100, axis=0)
    upper_bound = np.percentile(irf_matrix, (1 + confidence_level) / 2 * 100, axis=0)

    return original_irf, lower_bound, upper_bound

# Compute IRF and confidence intervals
irf_values, lower_bound, upper_bound = bootstrap_irf(data, shock_var, response_var, shock_size=1, horizons=horizons, num_bootstrap=num_bootstrap)



# Plot the IRF with confidence bands in your specified format
plt.figure(figsize=(8, 6))
plt.plot(irf_values, marker='o', linestyle='-', label='IRF')
plt.fill_between(range(horizons), lower_bound, upper_bound, color='gray', alpha=0.3, label=f"{int(confidence_level*100)}% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Add horizontal line at y=0
plt.xticks(np.arange(0, horizons, step=2))  # Set x-ticks as specified
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {shock_var} to {response_var}")
plt.xlabel("Time Periods After Shock")
plt.ylabel("Cumulative Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)

# Save and show plot
filename = "Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf"
plt.savefig(filename, bbox_inches='tight')
plt.show()

shock_var = 'FED News'  # Variable receiving the shock
response_var = 'GBPUSD'  # Variable whose response is measured
horizons = 48  # Number of periods to forecast

# Compute the IRF using local projections
irf_values = local_projection_irf(data, shock_var, response_var, shock_size=1, horizons=horizons)
print(irf_values)

#confidence bands
num_bootstrap = 1000  # Number of bootstrap samples
confidence_level = 0.95  # Desired confidence level (e.g., 95%)

# Function to compute IRF with bootstrapping for confidence intervals
def bootstrap_irf(data, shock_var, response_var, shock_size=1, horizons=10, num_bootstrap=1000):
    irf_matrix = []

    # Compute the original IRF
    original_irf = local_projection_irf(data, shock_var, response_var, shock_size=shock_size, horizons=horizons)

    # Run bootstrapping
    for _ in range(num_bootstrap):
        # Resample data with replacement
        boot_data = data.sample(frac=1, replace=True)

        # Compute IRF on bootstrapped data
        boot_irf = local_projection_irf(boot_data, shock_var, response_var, shock_size=shock_size, horizons=horizons)
        irf_matrix.append(boot_irf)

    # Convert to numpy array for easy manipulation
    irf_matrix = np.array(irf_matrix)

    # Calculate percentiles for confidence intervals
    lower_bound = np.percentile(irf_matrix, (1 - confidence_level) / 2 * 100, axis=0)
    upper_bound = np.percentile(irf_matrix, (1 + confidence_level) / 2 * 100, axis=0)

    return original_irf, lower_bound, upper_bound

# Compute IRF and confidence intervals
irf_values, lower_bound, upper_bound = bootstrap_irf(data, shock_var, response_var, shock_size=1, horizons=horizons, num_bootstrap=num_bootstrap)



# Plot the IRF with confidence bands in your specified format
plt.figure(figsize=(8, 6))
plt.plot(irf_values, marker='o', linestyle='-', label='IRF')
plt.fill_between(range(horizons), lower_bound, upper_bound, color='gray', alpha=0.3, label=f"{int(confidence_level*100)}% Confidence Interval")
plt.axhline(0, color='black', linestyle='--')  # Add horizontal line at y=0
plt.xticks(np.arange(0, horizons, step=2))  # Set x-ticks as specified
plt.title(f"Cumulative Generalized Impulse Response Function (GIRF) from {shock_var} to {response_var}")
plt.xlabel("Time Periods After Shock")
plt.ylabel("Cumulative Response of GBP/USD (in %)")
plt.legend()
plt.grid(True)

# Save and show plot
filename = "Cumulative_Generalized_Impulse_Response_function_of_GBPUSD_to_Stock_Market_News.pdf"
plt.savefig(filename, bbox_inches='tight')
plt.show()

"""# Noncumulative irfs of GBP/USD"""

irf =results.irf()
print(irf.irfs)

results.fevd(48).plot(figsize=(30,30));
print("FEVD")

import statsmodels.api as sm
results_summary = results.summary()

fevd

#!pip install rpy2==3.5.1
#%load_ext rpy2.ipython

#%%R

#plot(results.fevd(20), col=2:15)

#gc = results.test_causality('x', ['x', 'y'], kind='f')
#gc.summary()

# forecast
results.plot_forecast(10);

"""## Bootstrapping"""

# import pandas as pd
# import numpy as np
# from statsmodels.tsa.api import VAR
# n_bootstraps = 1000  # Number of bootstrap samples
# steps = 48  # Number of steps for the IRF
# def bootstrap_irfs(model, n_bootstraps=1000, steps=48):
#     n_obs = model.nobs
#     n_vars = model.endog.shape[1]  # Get the number of endogenous variables
#     bootstrapped_irfs = np.zeros((n_bootstraps, steps, n_vars, n_vars))  # Initialize for all bootstraps

#     # Convert endog to DataFrame if it is a NumPy array
#     endog_df = pd.DataFrame(model.endog)

#     for i in range(n_bootstraps):
#         # Generate bootstrap samples from DataFrame
#         bootstrap_sample = endog_df.sample(n=n_obs, replace=True).values
#         boot_model = VAR(bootstrap_sample)
#         boot_results = boot_model.fit(7) #maxlags=8, ic='aic'

#         # Get the IRFs
#         irf = boot_results.irf(steps)
#         bootstrapped_irfs[i] = irf.orth_irfs[1:]  # Store the orthogonalized IRFs excluding the zero time step

#     return bootstrapped_irfs

# bootstrapped_irfs = bootstrap_irfs(results, n_bootstraps, steps)
# def calculate_confidence_intervals(bootstrapped_irfs):
#     lower_bound = np.percentile(bootstrapped_irfs, 2.5, axis=0)
#     upper_bound = np.percentile(bootstrapped_irfs, 97.5, axis=0)
#     mean_irf = np.mean(bootstrapped_irfs, axis=0)
#     return mean_irf, lower_bound, upper_bound

# mean_irf, lower_bound, upper_bound = calculate_confidence_intervals(bootstrapped_irfs)
# import matplotlib.pyplot as plt

# # Calculate the mean IRF across bootstraps
# mean_irf = np.mean(bootstrapped_irfs, axis=0)  # Shape should be (steps, n_vars, n_vars)

# # Calculate the confidence intervals for the first variable (index 0) across all time steps
# lower_bound = np.percentile(bootstrapped_irfs[:, :, 0, :], 2.5, axis=0)  # Shape (steps, n_vars)
# upper_bound = np.percentile(bootstrapped_irfs[:, :, 0, :], 97.5, axis=0)  # Shape (steps, n_vars)

# # Time points for plotting (make sure it matches the number of steps)
# time_points = np.arange(mean_irf.shape[0])  # This should give an array of shape (steps,)

# # Plot the results
# plt.figure(figsize=(10, 5))
# plt.plot(time_points, mean_irf[:, 0], label='Mean IRF', color='blue')
# plt.fill_between(time_points, lower_bound[:, 0], upper_bound[:, 0], color='blue', alpha=0.3, label='95% CI')
# plt.title('Bootstrapped Impulse Response Function with Confidence Intervals')
# plt.xlabel('Time Steps')
# plt.ylabel('IRF')
# plt.legend()
# plt.show()

# import pandas as pd
# import numpy as np
# from statsmodels.tsa.api import VAR
# import matplotlib.pyplot as plt

# n_bootstraps = 1000  # Number of bootstrap samples
# steps = 48  # Number of steps for the IRF

# def bootstrap_irfs(model, n_bootstraps=1000, steps=48):
#     n_obs = model.nobs
#     n_vars = model.endog.shape[1]  # Get the number of endogenous variables
#     bootstrapped_irfs = np.zeros((n_bootstraps, steps, n_vars, n_vars))  # Initialize for all bootstraps

#     # Convert endog to DataFrame if it is a NumPy array
#     endog_df = pd.DataFrame(model.endog)

#     for i in range(n_bootstraps):
#         # Generate bootstrap samples from DataFrame
#         bootstrap_sample = endog_df.sample(n=n_obs, replace=True).values
#         boot_model = VAR(bootstrap_sample)
#         boot_results = boot_model.fit(7)  # maxlags=8, ic='aic'

#         # Get the IRFs
#         irf = boot_results.irf(steps)
#         bootstrapped_irfs[i] = irf.orth_irfs[1:]  # Store the orthogonalized IRFs excluding the zero time step

#     return bootstrapped_irfs

# # Assuming `results` is your fitted VAR model results
# bootstrapped_irfs = bootstrap_irfs(results, n_bootstraps, steps)

# def calculate_confidence_intervals(bootstrapped_irfs):
#     mean_irf = np.mean(bootstrapped_irfs, axis=0)
#     lower_bound = np.percentile(bootstrapped_irfs, 2.5, axis=0)
#     upper_bound = np.percentile(bootstrapped_irfs, 97.5, axis=0)
#     return mean_irf, lower_bound, upper_bound

# # Calculate the mean IRF and confidence intervals for the first variable responding to the shock in the tenth variable
# mean_irf, lower_bound, upper_bound = calculate_confidence_intervals(bootstrapped_irfs)

# # Time points for plotting (make sure it matches the number of steps)
# time_points = np.arange(mean_irf.shape[0])  # This should give an array of shape (steps,)

# # Plot the results for the first variable's response to the 10th variable
# plt.figure(figsize=(10, 5))
# plt.plot(time_points, mean_irf[:, 0, 9], label='Mean IRF (Var1 to Var10)', color='blue')  # First variable's response to the 10th variable
# plt.fill_between(time_points, lower_bound[:, 0, 9], upper_bound[:, 0, 9], color='blue', alpha=0.3, label='95% CI')
# plt.title('Bootstrapped Impulse Response Function: Var1 Response to Var10')
# plt.xlabel('Time Steps')
# plt.ylabel('IRF')
# plt.legend()
# plt.show()

# plt.figure(figsize=(10, 5))
# plt.plot(time_points, mean_irf[:, 0, 12], label='Mean IRF (Var1 to Var10)', color='blue')  # First variable's response to the 10th variable
# plt.fill_between(time_points, lower_bound[:, 0, 12], upper_bound[:, 0, 12], color='blue', alpha=0.3, label='95% CI')
# plt.title('Bootstrapped Impulse Response Function: Var1 Response to Var10')
# plt.xlabel('Time Steps')
# plt.ylabel('IRF')
# plt.legend()
# plt.show()

# plt.figure(figsize=(10, 5))
# plt.plot(time_points, mean_irf[:, 0, 13], label='Mean IRF (Var1 to Var10)', color='blue')  # First variable's response to the 10th variable
# plt.fill_between(time_points, lower_bound[:, 0, 13], upper_bound[:, 0, 13], color='blue', alpha=0.3, label='95% CI')
# plt.title('Bootstrapped Impulse Response Function: Var1 Response to Var10')
# plt.xlabel('Time Steps')
# plt.ylabel('IRF')
# plt.legend()
# plt.show()

"""# Noncumulative irfs

Please note that we only present the cummulative irfs in the appendix, which r allows to be calculated. These irfs here are not cummulative and are not presented in my study.
"""

# Colors for plotting
colors_news = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]

irf = results.irf(48)

#print(irf.irfs)
fig=irf.plot();
fig.set_size_inches(23.5, 15.5)
print("IRF")

from pickle import TRUE
irf.plot(response='GBPUSD', impulse = 'Stock Market News', orth=TRUE, signif=0.05) #, color=colors_news[0], ci_color='red'
print("IRF")

irf.plot(response='GBPUSD', impulse = 'Economic Development News', orth=TRUE, signif=0.05) #, color=colors_news[1], ci_color='red'
print("IRF")

#irf.plot(response='GBPUSD', impulse = 'FED News')
irf.plot(response='GBPUSD', impulse = 'FED News', orth=TRUE, signif=0.05); #, color=colors_news[2], ci_color='red'

print("IRF")

irf.plot(response='GBPUSD', impulse = 'Micro Finance News', orth=TRUE, signif=0.05) #, color=colors_news[3], ci_color='red'
print("IRF")

irf.plot(response='GBPUSD', impulse = 'International Trade News', orth=TRUE, signif=0.05) #, color=colors_news[4], ci_color='red'
print("IRF")

"""Please note that we only present the cummulative irfs in the appendix, which r allows to be calculated. These irfs here are not cummulative and are not presented in my study."""

irf = results.irf(48)

#print(irf.irfs)
fig=irf.plot();
fig.set_size_inches(23.5, 15.5)
print("IRF")

from pickle import TRUE
# Colors for plotting
colors_news = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]

irf.plot(response='GBPUSD', impulse = 'Stock Market News', orth=TRUE, signif=0.05) #, color=colors_news[0], ci_color='red'
print("IRF")

irf.plot(response='GBPUSD', impulse = 'Economic Development News', orth=TRUE, signif=0.05, color=colors_news[1], ci_color='red')
print("IRF")

#irf.plot(response='GBPUSD', impulse = 'FED News')
irf.plot(response='GBPUSD', impulse = 'FED News', orth=TRUE, signif=0.05, color=colors_news[2], ci_color='red');

print("IRF")

irf.plot(response='GBPUSD', impulse = 'Micro Finance News', orth=TRUE, signif=0.05, color=colors_news[3], ci_color='red')
print("IRF")

irf.plot(response='GBPUSD', impulse = 'International Trade News', orth=TRUE, signif=0.05, color=colors_news[4], ci_color='red')
print("IRF")

import matplotlib.pyplot as plt
plt.figure(figsize=(185,65))
#fig.set_size_inches(135,75)
irf.plot(response='GBPUSD')
print('irf')
plt.show()

from pickle import TRUE
irf.plot(response='GBPUSD', impulse = 'Stock Market News', orth=TRUE, signif=0.1)
print("IRF")

irf.plot(response='GBPUSD', impulse = 'Economic Development News', orth=TRUE, signif=0.1)
print("IRF")

#irf.plot(response='GBPUSD', impulse = 'FED News')
irf.plot(response='GBPUSD', impulse = 'FED News', orth=TRUE, signif=0.1);

print("IRF")

from pickle import TRUE
#irf.plot(response='GBPUSD', impulse = 'Micro Finance News')
irf.plot(response='GBPUSD', impulse = 'Micro Finance News', orth=TRUE, signif=0.1);
print("IRF")

irf.plot(response='GBPUSD', impulse = 'International Trade News', orth=TRUE, signif=0.05)
print("IRF")

import numpy as np

# Assuming 'irf' is your impulse response functions obtained from your model

# Get the number of time periods and the number of variables
n_periods =10
n_variables =16

# Initialize an array to store cumulative impulse response functions
cirf = np.zeros_like(irf)

# Calculate cumulative impulse response functions
for t in range(1, n_periods):
    cirf[t] = cirf[t-1] + irf[t]

# Plot cumulative impulse response functions

variable_index = 0  # Assuming you want to plot the first variable
plt.plot(cirf[:, variable_index, :])
plt.title('Cumulative Impulse Response Functions')
plt.xlabel('Time')
plt.ylabel('Cumulative Response')
plt.show()

import numpy as np

# Assuming 'irf' is your impulse response functions obtained from your model

# Get the number of time periods and the number of variables
n_periods =10
n_variables =16

# Initialize an array to store cumulative impulse response functions
cirf = np.zeros_like(irf)

# Calculate cumulative impulse response functions
for t in range(1, n_periods):
    cirf[t] = cirf[t-1] + irf[t]

# Plot cumulative impulse response functions

variable_index = 0  # Assuming you want to plot the first variable
plt.plot(cirf[:, variable_index, :])
plt.title('Cumulative Impulse Response Functions')
plt.xlabel('Time')
plt.ylabel('Cumulative Response')
plt.show()

fevd_var.plot(0)

colors = ["#00338D", "#005EB8", "#0091DA", "#483698", "#470A68", "#009A44", "#43B02A", "#765341", "#C6007E", "#6D2077", "#00A3A1", "#ff9933", "#BC204B", "#333333", "#EAAA00", "#F68D2E"]
fevd_results.plot(0)

# Display the DataFrame in LaTeX format
#print(data_summary.to_latex())

# Export the DataFrame to a LaTeX file
#df_summary.to_latex("summary.tex")

"""# Cummulative IRFs"""

# PLEASE NOTE THAT THERE CAN BE A BUG HERE, AS IT WAS REPORTED IN THE INTERNET. I CALCULATE AND PLOT CUMMULATIVE IRFs in R

irf.cum_effects[1]

m,n,r = irf.cum_effects.shape
out_arr = np.column_stack((np.repeat(np.arange(m),n),irf.cum_effects.reshape(m*n,-1)))
cirf_df = pd.DataFrame(out_arr)
cirf_df

x=irf.cum_effect_stderr(48)
m,n,r = irf.cum_effect_stderr(48).shape
out_arr = np.column_stack((np.repeat(np.arange(m),n),x.reshape(m*n,-1)))
cirf_stderr_df = pd.DataFrame(out_arr)
cirf_stderr_df

# cummulative irfs as numbers
print(irf.cum_effects)

import pandas as pd

# Convert cumulative effects to a DataFrame
cum_effects_df = pd.DataFrame(irf.cum_effects.reshape(-1, irf.cum_effects.shape[-1]))

# Print the DataFrame
print(cum_effects_df)

# import pandas as pd
# import matplotlib.pyplot as plt

# # Assuming cum_effects_df is already defined
# # cum_effects_df = pd.DataFrame(irf.cum_effects.reshape(-1, irf.cum_effects.shape[-1]))

# # Define the names of the response and impulse variables
# response_variable = cum_effects_df.columns[0]  # GBPUSD (first variable)
# impulse_variable = cum_effects_df.columns[9]    # Stock Market News (tenth variable)

# # Plotting
# plt.figure(figsize=(12, 6))
# #plt.plot(cum_effects_df.index, cum_effects_df[response_variable], label='GBPUSD', color='blue', linestyle='-', linewidth=2)
# plt.plot(cum_effects_df.index, cum_effects_df[impulse_variable], label='Stock Market News', color='orange', linestyle='--', linewidth=2)
# plt.title(f'Cumulative Effects of {impulse_variable} on {response_variable}')
# plt.xlabel('Time')
# plt.ylabel('Cumulative Effect')
# plt.axhline(0, color='gray', linestyle='--', linewidth=1)  # Reference line at y=0
# plt.legend()
# plt.grid()
# plt.tight_layout()
# plt.show()

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 9    # Index for Stock Market News

# Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

cumulative_effects

"""## hand calculated"""

# Assuming irf.cum_effects is a numpy array

# Colors for plotting
colors_news = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]


response_index = 0  # Index for GBPUSD
impulse_index = 9    # Index for Stock Market News

# NOTGENERALIZED Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

# #GENERALIZED generalized cumulative impulse response function
# girf = irf.orth_irfs  # Replace with actual GIRF computation
# girf_response = girf[:, response_index]
# # Calculate the cumulative IRF
# cumulative_effects = np.cumsum(girf_response)


import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Response of GBP/USD to Stock Market News Shock', color=colors_news[0]) #Cumulative Effect of Stock Market News on GBPUSD
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[0], alpha=0.4, label='95% Confidence Interval') #
# # Plot lower and upper bounds as dotted lines
# plt.plot(range(len(cumulative_effects)), lower_ci, color=colors_news[0], linestyle='--', linewidth=0.01)
# plt.plot(range(len(cumulative_effects)), upper_ci, color=colors_news[0], linestyle='--', linewidth=0.01)

# Add titles and labels
plt.title('Cumulative Impulse Response from Stock Market News to GBP/USD') #Cumulative Effects of Stock Market News on GBPUSD
plt.xlabel('Time Periods After Shock')
plt.ylabel('Response of GBP/USD (in %)') #Cumulative Effect
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

# Show the plot
plt.savefig("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
plt.show()
files.download("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf")

irf.orth_irfs[9, 0]

# Assuming you have a list of variable names to confirm indexing
variable_names = ["GBPUSD", "Variable2", "Var3", "Variable4", "Variable5", "Variable6", "Variable7", "Variable8", "Variable9", "Stock Market News"]
print("Response Index for GBP/USD:", response_index, " - ", variable_names[response_index])
print("Impulse Index for Stock Market News:", impulse_index, " - ", variable_names[impulse_index])

# Assuming irf.cum_effects is a numpy array

# Colors for plotting
colors_news = ["#6D2077", "#00A3A1", "#ff9933",
          "#BC204B", "#333333", "#6B8E23", "#808000"]


response_index = 0  # Index for GBPUSD
impulse_index = 9    # Index for Stock Market News

#GENERALIZED generalized cumulative impulse response function
girf = irf.orth_irfs  # Replace with actual GIRF computation
girf_response = girf[:, response_index, impulse_index]#girf[impulse_index, response_index]
# Calculate the cumulative IRF
cumulative_effects = np.cumsum(girf_response)


import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Generalized Response of GBP/USD to Stock Market News Shock', color=colors_news[0]) #Cumulative Effect of Stock Market News on GBPUSD
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[0], alpha=0.2, label='95% Confidence Interval') #'lightgray'
# # Plot lower and upper bounds as dotted lines
# plt.plot(range(len(cumulative_effects)), lower_ci, color=colors_news[0], linestyle='--', linewidth=0.01)
# plt.plot(range(len(cumulative_effects)), upper_ci, color=colors_news[0], linestyle='--', linewidth=0.01)

# Add titles and labels
plt.title('Cumulative Generalized Impulse Response from Stock Market News to GBP/USD') #Cumulative Effects of Stock Market News on GBPUSD
plt.xlabel('Time Periods After Shock')
plt.ylabel('Cumulative Generalized Response of GBP/USD (in %)') #Cumulative Effect
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

# Show the plot
plt.savefig("Cumulative Generalized Impulse Response function of GBPUSD to Stock Market News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
plt.show()
files.download("Cumulative Generalized Impulse Response function of GBPUSD to Stock Market News.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 10   # Index for Stock Market News

# Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

# # Assuming you need to call the method
# error_bands = irf.cum_errband_mc()  # Call the method; adjust if parameters are needed

# # Now extract lower and upper bounds from the returned values
# lower_bound = error_bands[0][:, response_index, impulse_index]  # Adjust indexing based on returned structure
# upper_bound = error_bands[1][:, response_index, impulse_index]  # Adjust indexing based on returned structure


import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# # Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Impulse Response from Economic Development News to GBP/USD', color=colors_news[1])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Upper bound (95% CI)


plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[1], alpha=0.4, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Impulse Response from Economic Development News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

# Show the plot
plt.savefig("Cumulative Impulse Response function of GBPUSD to Economic Development News.pdf", bbox_inches='tight', pad_inches=0.001)

plt.show()
files.download("Cumulative Impulse Response function of GBPUSD to Economic Development News.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 10   # Index for Stock Market News

#GENERALIZED generalized cumulative impulse response function
girf = irf.orth_irfs  # Replace with actual GIRF computation
girf_response = girf[:, response_index, impulse_index]#girf[impulse_index, response_index]
# Calculate the cumulative IRF
cumulative_effects = np.cumsum(girf_response)


import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# # Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Generalized Impulse Response from Economic Development News to GBP/USD', color=colors_news[1])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Upper bound (95% CI)


plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[1], alpha=0.2, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Generalized Impulse Response from Economic Development News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Cumulative Generalized Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

# Show the plot
plt.savefig("Cumulative Generalized Impulse Response function of GBPUSD to Economic Development News.pdf", bbox_inches='tight', pad_inches=0.001)

plt.show()
files.download("Cumulative Generalized Impulse Response function of GBPUSD to Economic Development News.pdf")

#print(dir(irf))

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 11    # Index for Stock Market News

# Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Impulse Response from FED News to GBP/USD', color=colors_news[2])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci,color=colors_news[2], alpha=0.4, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Impulse Response from FED News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()
plt.savefig("Cumulative Impulse Response function of GBPUSD to FED News.pdf", bbox_inches='tight', pad_inches=0.001)

# Show the plot
plt.show()
files.download("Cumulative Impulse Response function of GBPUSD to FED News.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 11    # Index for Stock Market News



#GENERALIZED generalized cumulative impulse response function
girf = irf.orth_irfs  # Replace with actual GIRF computation
girf_response =  girf[:, response_index, impulse_index]#girf[impulse_index, response_index]
# Calculate the cumulative IRF
cumulative_effects = np.cumsum(girf_response)

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)

# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Generalized Impulse Response from FED News to GBP/USD', color=colors_news[2])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci,color=colors_news[2], alpha=0.4, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Generalized Impulse Response from FED News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Cumulative Generalized Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()
plt.savefig("Cumulative Generalized Impulse Response function of GBPUSD to FED News.pdf", bbox_inches='tight', pad_inches=0.001)

# Show the plot
plt.show()
files.download("Cumulative Generalized Impulse Response function of GBPUSD to FED News.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 12   # Index for Stock Market News

# Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)
# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Impulse Response from Microeconomics News to GBP/USD', color=colors_news[3])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[3], alpha=0.4, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Impulse Response from Microeconomics News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()
plt.savefig("Cumulative Impulse Response function of GBPUSD to Microeconomics News.pdf", bbox_inches='tight', pad_inches=0.001)
plt.show()
files.download("Cumulative Impulse Response function of GBPUSD to Microeconomics News.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 12   # Index for Stock Market News

#GENERALIZED generalized cumulative impulse response function
girf = irf.orth_irfs  # Replace with actual GIRF computation
girf_response =  girf[:, response_index, impulse_index]#girf[impulse_index, response_index]
# Calculate the cumulative IRF
cumulative_effects = np.cumsum(girf_response)

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)
# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Generalized Impulse Response from Microeconomics News to GBP/USD', color=colors_news[3])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[3], alpha=0.2, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Generalized Impulse Response from Microeconomics News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Cumulative Generalized Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()
plt.savefig("Cumulative Generalized Impulse Response function of GBPUSD to Microeconomics News.pdf", bbox_inches='tight', pad_inches=0.001)
plt.show()
files.download("Cumulative Generalized Impulse Response function of GBPUSD to Microeconomics News.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 13    # Index for Stock Market News

# Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)
# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Impulse Response from International Trade News to GBP/USD', color=colors_news[4])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[4], alpha=0.4, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Impulse Response from International Trade News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

plt.savefig("Cumulative Impulse Response function of GBPUSD to International Trade.pdf", bbox_inches='tight', pad_inches=0.001)
# Show the plot
plt.show()
files.download("Cumulative Impulse Response function of GBPUSD to International Trade.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 13    # Index for Stock Market News

#GENERALIZED generalized cumulative impulse response function
girf = irf.orth_irfs  # Replace with actual GIRF computation
girf_response =  girf[:, response_index, impulse_index]#girf[impulse_index, response_index]
# Calculate the cumulative IRF
cumulative_effects = np.cumsum(girf_response)

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(11.69,8.27), dpi=600)
# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Generalized Impulse Response from International Trade News to GBP/USD', color=colors_news[4])
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Lower bound (95% CI)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))   # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color=colors_news[4], alpha=0.2, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Generalized Impulse Response from International Trade News to GBP/USD')
plt.xlabel('Time Periods After Shock')
plt.ylabel('Cumulative Generalized Response of GBP/USD (in %)')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

plt.savefig("Cumulative Generalized Impulse Response function of GBPUSD to International Trade.pdf", bbox_inches='tight', pad_inches=0.001)
# Show the plot
plt.show()
files.download("Cumulative Generalized Impulse Response function of GBPUSD to International Trade.pdf")

# Assuming irf.cum_effects is a numpy array
response_index = 0  # Index for GBPUSD
impulse_index = 10    # Index for Stock Market News

# Extracting cumulative effects
cumulative_effects = irf.cum_effects[:, response_index, impulse_index]

import pandas as pd
import matplotlib.pyplot as plt

# Create a plot
plt.figure(figsize=(12, 6))

# Plot the cumulative effects for GBPUSD in response to Stock Market News
plt.plot(cumulative_effects, label='Cumulative Effect of Economic Policy Uncertainty US on GBPUSD', color='blue')
lower_ci = cumulative_effects - 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0]))  # Lower bound (95% CI) np.std(cumulative_effects)
upper_ci = cumulative_effects + 1.96 * (np.std(cumulative_effects)/ np.sqrt(cumulative_effects.shape[0])) # Upper bound (95% CI)
plt.fill_between(range(len(cumulative_effects)), lower_ci, upper_ci, color='blue', alpha=0.2, label='95% Confidence Interval')

# Add titles and labels
plt.title('Cumulative Effects of Economic Policy Uncertainty US on GBPUSD')
plt.xlabel('Time Periods')
plt.ylabel('Cumulative Effect')
plt.axhline(0, color='black', linewidth=0.5, linestyle='--')  # Reference line at 0
plt.legend()
plt.grid()

# Show the plot
plt.show()

#from pickle import TRUE
# the horizontal line is the long-run impact of the impulse on the response variable. This value is often derived from the cumulative effects calculated after an impulse is applied and can indicate the eventual steady-state response to a shock.
from google.colab import files
import matplotlib.ticker as mtick
#getAnywhere(irf.plot_cum_effects)
plt=irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
files.download("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf")

plt=irf.plot_cum_effects(response='GBPUSD', impulse='Economic Development News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Cumulative Impulse Response function of GBPUSD to Economic Development News.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Cumulative Impulse Response function of GBPUSD to Economic Development News.pdf")

plt=irf.plot_cum_effects(response='GBPUSD', impulse='FED News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Cumulative Impulse Response function of GBPUSD to FED News.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Cumulative Impulse Response function of GBPUSD to FED News.pdf")

plt=irf.plot_cum_effects(response='GBPUSD', impulse='Micro Finance News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Cumulative Impulse Response function of GBPUSD to Micro Finance News.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Cumulative Impulse Response function of GBPUSD to Micro Finance News.pdf")

plt=irf.plot_cum_effects(response='GBPUSD', impulse='International Trade News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Cumulative Impulse Response function of GBPUSD to International Trade.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Cumulative Impulse Response function of GBPUSD to International Trade.pdf")

#from pickle import TRUE
from google.colab import files
import matplotlib.ticker as mtick
#getAnywhere(irf.plot_cum_effects)
plt=irf.plot(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Impulse Response function of GBPUSD to Stock Market News.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
files.download("Impulse Response function of GBPUSD to Stock Market News.pdf")

plt=irf.plot(response='GBPUSD', impulse='Economic Development News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Impulse Response function of GBPUSD to Economic Development News.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Impulse Response function of GBPUSD to Economic Development News.pdf")

plt=irf.plot(response='GBPUSD', impulse='FED News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Impulse Response function of GBPUSD to FED News.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Impulse Response function of GBPUSD to FED News.pdf")

plt=irf.plot(response='GBPUSD', impulse='Micro Finance News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Impulse Response function of GBPUSD to Micro Finance News.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Impulse Response function of GBPUSD to Micro Finance News.pdf")

plt=irf.plot(response='GBPUSD', impulse='International Trade News', orth=TRUE, signif=0.05)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
plt.savefig("Impulse Response function of GBPUSD to International Trade.pdf", bbox_inches='tight', pad_inches=0.001)
files.download("Impulse Response function of GBPUSD to International Trade.pdf")

#from pickle import TRUE
from google.colab import files
import matplotlib.ticker as mtick
#getAnywhere(irf.plot_cum_effects)
plt=irf.plot_cum_effects(response='GBPUSD', impulse='GBPUSD', orth=TRUE, signif=0.1) #plot_stderr= False

#plt=irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.1) #plot_stderr= False
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
# plt.savefig("Cumulative Impulse Response function of GBPUSD to Stock Market News_0.1.pdf", bbox_inches='tight', pad_inches=0.001) #in order not to get the borders when downloading.
# files.download("Cumulative Impulse Response function of GBPUSD to Stock Market News_0.1.pdf")

# plt=irf.plot_cum_effects(response='GBPUSD', impulse='Economic Development News', orth=TRUE, signif=0.1)
# plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
# plt.savefig("Cumulative Impulse Response function of GBPUSD to Economic Development News_0.1.pdf", bbox_inches='tight', pad_inches=0.001)
# files.download("Cumulative Impulse Response function of GBPUSD to Economic Development News_0.1.pdf")

# plt=irf.plot_cum_effects(response='GBPUSD', impulse='FED News', orth=TRUE, signif=0.1)
# plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
# plt.savefig("Cumulative Impulse Response function of GBPUSD to FED News_0.1.pdf", bbox_inches='tight', pad_inches=0.001)
# files.download("Cumulative Impulse Response function of GBPUSD to FED News_0.1.pdf")

# plt=irf.plot_cum_effects(response='GBPUSD', impulse='Micro Finance News', orth=TRUE, signif=0.1)
# plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
# plt.savefig("Cumulative Impulse Response function of GBPUSD to Micro Finance News_0.1.pdf", bbox_inches='tight', pad_inches=0.001)
# files.download("Cumulative Impulse Response function of GBPUSD to Micro Finance News_0.1.pdf")

# plt=irf.plot_cum_effects(response='GBPUSD', impulse='International Trade News', orth=TRUE, signif=0.1)
# plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))
# plt.savefig("Cumulative Impulse Response function of GBPUSD to International Trade_0.1.pdf", bbox_inches='tight', pad_inches=0.001)
# files.download("Cumulative Impulse Response function of GBPUSD to International Trade_0.1.pdf")

#from pickle import TRUE
import matplotlib.ticker as mtick
from google.colab import files
plt=irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.1)
plt

#from pickle import TRUE
from google.colab import files
plt=irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.2)
plt.savefig("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf")
files.download("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf")

plt=irf.plot_cum_effects(response='GBPUSD', impulse='Economic Development News', orth=TRUE, signif=0.2)
plt.savefig("Cumulative Impulse Response function of GBPUSD to Economic Development News.pdf")
files.download("Cumulative Impulse Response function of GBPUSD to Economic Development News.pdf")

fig =irf.plot_cum_effects(orth=TRUE);
fig.set_size_inches(23.5, 15.5)
fig
#This is matrix of 1.USDEUR	CPI_US	2.CPI_EU	3.Money_Market_Rate_US	4.Money_Market_Rate_EU	5.IPI_US	6.IPI_EU	7.M2_US	8.M2_EU	Stock Market News	Economic Development News	FED News	Micro Finance News	International Trade News
print("All CIRFs")

from pickle import FALSE
fig=irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE)
print()
fig.plot()

irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.1)
print("CIRF")

from google.colab import files
plt=irf.plot_cum_effects(response='GBPUSD', impulse='Stock Market News', orth=TRUE, signif=0.1)
plt.savefig("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf")
files.download("Cumulative Impulse Response function of GBPUSD to Stock Market News.pdf")

irf.plot_cum_effects(response='GBPUSD', impulse='Economic Development News', orth=TRUE, signif=0.1)
print("CIRF")

irf.plot_cum_effects(response='GBPUSD', impulse='FED News', orth=TRUE, signif=0.1)
print("CIRF")

irf.plot_cum_effects(response='GBPUSD', impulse='Micro Finance News', orth=TRUE, signif=0.1)
print("CIRF")

irf.plot_cum_effects(response='GBPUSD', impulse='International Trade News', orth=TRUE, signif=0.1)
print("CIRF")

##cirfs = pd.DataFrame(irf.cum_effects[:, 1])
##cirfs

"""#Grangercausality tests"""

# Granger causality of each signle lag
from statsmodels.tsa.stattools import grangercausalitytests

print("Topic 1")
data['allnews'] = data['Stock Market News'] + data['Economic Development News'] + data['FED News'] +data['Micro Finance News'] +data['International Trade News']
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "Stock Market News"]], [i])
  print(i)
  print(gc_res)

print("Topic 2")
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "Economic Development News"]], [i])
  print(i)
  print(gc_res)

print("Topic 3")
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "FED News"]], [i])
  print(i)
  print(gc_res)
print("Topic 4")
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "Micro Finance News"]], [i])
  print(i)
  print(gc_res)
print("Topic 5")
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "International Trade News"]], [i])
  print(i)
  print(gc_res)
print("EPU_US")
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "EPU_US"]], [i])
  print(i)
  print(gc_res)
print("EPU_UK")
for i in range (1,8):
  gc_res = grangercausalitytests(data[["GBPUSD", "EPU_UK"]], [i])
  print(i)
  print(gc_res)

from statsmodels.tsa.stattools import grangercausalitytests

# Function to format significance levels
def format_significance(p_value):
    if p_value < 0.01:
        return "***"
    elif p_value < 0.05:
        return "**"
    elif p_value < 0.1:
        return "*"
    else:
        return ""

# Define topics and their respective columns
topics = {
    "Topic 1": "Stock Market News",
    "Topic 2": "Economic Development News",
    "Topic 3": "FED News",
    "Topic 4": "Micro Finance News",
    "Topic 5": "International Trade News",
    "EPU_US": "EPU_US",
    "EPU_UK": "EPU_UK"
}

# Add a combined news column
data['allnews'] = (
    data['Stock Market News'] +
    data['Economic Development News'] +
    data['FED News'] +
    data['Micro Finance News'] +
    data['International Trade News'] +
    data['EPU_US'] +
    data['EPU_UK']
)

# Perform Granger causality tests for each topic
for topic, column in topics.items():
    print(topic)
    print("-" * 40)
    for lag in range(1, 8):
        gc_res = grangercausalitytests(data[["GBPUSD", column]], [lag], verbose=False)
        f_test = gc_res[lag][0]['params_ftest'][0]
        p_value = gc_res[lag][0]['params_ftest'][1]
        significance = format_significance(p_value)
        print(f"Lag {lag}: F-test = {f_test:.4f}, p-value = {p_value:.4g} {significance}")
    print()

# from statsmodels.tsa.stattools import grangercausalitytests
# maxlag=7 #becuase we got this value before. We are not suppose to add 1 to it
# test = 'ssr_chi2test'
# def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):
#     """Check Granger Causality of all possible combinations of the Time series.
#     The rows are the response variable, columns are predictors. The values in the table
#     are the P-Values. P-Values lesser than the significance level (0.05), implies
#     the Null Hypothesis that the coefficients of the corresponding past values is
#     zero, that is, the X does not cause Y can be rejected.

#     data      : pandas dataframe containing the time series variables
#     variables : list containing names of the time series variables.
#     """
#     df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
#     for c in df.columns:
#         for r in df.index:
#             test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)
#             p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
#             if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')
#             min_p_value = np.min(p_values)
#             df.loc[r, c] = min_p_value
#     df.columns = [var + '_x' for var in variables]
#     df.index = [var + '_y' for var in variables]
#     return df
# granger = grangers_causation_matrix(data, variables = data.columns)
# #grangers_causation_matrix(data, variables = data.columns)
# #print(granger.to_latex())

data['allnews'] = data['Stock Market News'] + data['Economic Development News'] + data['FED News'] +data['Micro Finance News'] +data['International Trade News']
gc_res = grangercausalitytests(data[["allnews", "GBPUSD"]],24)
gc_res
data = data.drop("allnews", axis=1)
#FED News
#Micro Finance News
#International Trade News
gc_res

#gc_res = grangercausalitytests(data[["GBPUSD", "Stock Market News"]],7)
#gc_res = grangercausalitytests(data[["Stock Market News", "GBPUSD"]],7)
#gc_res = grangercausalitytests(data[["GBPUSD", "Economic Development News"]],7)
#gc_res = grangercausalitytests(data[["GBPUSD", "FED News"]],7)
#gc_res = grangercausalitytests(data[["GBPUSD", "Micro Finance News"]],7)
#gc_res = grangercausalitytests(data[["GBPUSD", "International Trade News"]],7)
#gc_res = grangercausalitytests(data[["GBPUSD", "EPU_US"]],7)
#gc_res = grangercausalitytests(data[["GBPUSD", "EPU_UK"]],7)
data['allnews'] = data['Stock Market News'] + data['Economic Development News'] + data['FED News'] +data['Micro Finance News'] +data['International Trade News']
gc_res = grangercausalitytests(data[["GBPUSD", "allnews"]],24)
gc_res
data = data.drop("allnews", axis=1)
#FED News
#Micro Finance News
#International Trade News
gc_res

gc_res = grangercausalitytests(data[["allnews", "GBPUSD"]],7)
gc_res
data = data.drop("allnews", axis=1)
#FED News
#Micro Finance News
#International Trade News
gc_res

#Instantaneous causality
import pandas as pd
from statsmodels.tsa.stattools import grangercausalitytests

def check_instantaneous_causality(data, var1, var2, max_lag=1, significance_level=0.05):
    """
    Perform Granger causality tests to check for instantaneous causality between two variables.

    Parameters:
    - data: DataFrame containing the time series data.
    - var1: Name of the first variable.
    - var2: Name of the second variable.
    - max_lag: Maximum lag to consider in the Granger causality tests (default is 1 for instantaneous causality).
    - significance_level: Significance level for the Granger causality tests (default is 0.05).

    Returns:
    - results: Dictionary containing the results of the Granger causality tests.
    """
    # Extract the variables from the DataFrame
    series1 = data[var1]
    series2 = data[var2]

    # Perform Granger causality tests
    results = grangercausalitytests(data[[var1, var2]], maxlag=1, verbose=False)

    # Extract p-values from the test results
    p_values = [results[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]

    # Check for significance based on the specified significance level
    significant = any(p_value < significance_level for p_value in p_values)

    # Print results
    print(f"Granger causality test results for {var1} causing {var2}:")
    for lag, p_value in enumerate(p_values, 1):
        print(f"  Lag {lag}: p-value = {p_value}")

    # Print conclusion
    if significant:
        print("Conclusion: There is evidence of instantaneous causality.")
    else:
        print("Conclusion: There is no evidence of instantaneous causality.")

    return results

# Example usage:
# Assuming you have a DataFrame 'data' containing time series data with columns 'var1' and 'var2'
# and you want to test for instantaneous causality between 'var1' and 'var2' at lag 1
# results = check_instantaneous_causality(data, 'var1', 'var2', max_lag=1, significance_level=0.05)
check_instantaneous_causality(data, 'Stock Market News', 'GBPUSD')

check_instantaneous_causality(data, 'GBPUSD', 'FED News')

"""# K-Fold Cross-Validation (Not necessary, as our main goal is not forecasting)"""

from statsmodels.tsa.api import VAR
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error

# Assuming 'data' is your input dataframe with variables in columns and time points in rows

# Define the number of folds for cross-validation
n_splits = 5

# Initialize a TimeSeriesSplit object
tscv = TimeSeriesSplit(n_splits=n_splits)

# Initialize lists to store evaluation metrics
rmse_list = []

# Iterate through the cross-validation folds
for train_index, test_index in tscv.split(data):
    # Split the data into training and testing sets
    train_data, test_data = data.iloc[train_index], data.iloc[test_index]

    # Fit the VAR model on the training set
    model = VAR(train_data)
    results = model.fit()

    # Forecast the values for the testing set
    forecast = results.forecast(train_data.values[-results.k_ar:], len(test_data))

    # Calculate RMSE for the forecast
    rmse = mean_squared_error(test_data, forecast, squared=False)
    rmse_list.append(rmse)

# Calculate the mean RMSE across all folds
mean_rmse = np.mean(rmse_list)

# Print the mean RMSE
print("Mean RMSE:", mean_rmse)

"""Mean RMSE: 0.061269223859123"""